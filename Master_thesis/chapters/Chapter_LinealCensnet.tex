\chapter{Natural Gas System Prediction Using Graph Neural Networks} \label{cap:lienal-censnet}

In recent years, the increasing complexity of natural gas transportation systems has driven the need for more efficient and scalable methods to estimate their operational behavior. Traditional optimization approaches, while accurate, can become computationally expensive, particularly when applied to large-scale or real-time scenarios. This chapter examines the application of Graph Neural Networks (GNNs) as a surrogate modeling technique for forecasting the operational state of natural gas networks. By leveraging the inherent graph structure of these systems, where nodes represent physical elements such as wells or demand points, and edges represent pipelines or compressors. The GNN model is trained to approximate nodal and edge-level variables with high accuracy. Special attention is given to incorporating physical constraints into the learning objective, particularly losses at both the node and edge levels, to ensure consistency with the underlying physics. The approach is validated on two test systems of varying complexity: a small 8-node network and a 63-node model representing the Colombian gas infrastructure. The results demonstrate that this topology-aware GNN framework can significantly reduce computation times while maintaining high prediction quality.



\section{Mathematical framework}

\subsection{Graph definition}


A graph $G$ is a mathematical structure that represents a set of interconnected objects. These objects are known as vertices (or nodes), denoted by the set $V$, and the connections between them are called edges (or arcs), denoted by the set $E$. Formally, a graph is defined as an ordered pair $G = (V, E)$, where $V$ is a non-empty set of vertices, and $E \subseteq \{(u, v) \mid u, v \in V(G), u \neq v\}$ is the set of edges, where each edge connects two distinct vertices \cite{Trudeau_2015}.

Graphs can be categorized based on the properties of their edges. An undirected graph has edges that do not have a direction, so the pair $(u, v) = (v, u)$ represents an edge that simply connects vertices $u$ and $v$. In contrast, in a directed graph (or digraph), each edge $(u, v) \in E$ has a direction, meaning it goes from vertex $u$ to vertex $v$. This implies that $(u, v) \neq (v, u)$ unless $u = v$ \cite{Bender_Williamson_2010}.

\begin{figure}[h]
    \centering
        \setlength\figurewidth{.4\textwidth}        
        \setlength\figureheight{0.26\textwidth} 
        \subfloat [Undirected graph] {\label{fig:undirected_graph_def}\resizebox{\figurewidth}{\figureheight}{\input{figures/Chapter_LinealCensnet/un_graph_def}}}
        \subfloat [Directed graph] {\label{fig:direc_graph_def}\resizebox{\figurewidth}{\figureheight}{\input{figures/Chapter_LinealCensnet/di_graph_def}}}
        % \includegraphics[width=0.45\textwidth]{figures/}
    \caption{Types of graphs}\label{fig:graph_definition}
\end{figure}

In \Cref{fig:graph_definition}, two graphs are represented, each composed of four nodes labeled $1$, $2$, $3$, and $4$, and six edges labeled $A$, $B$, $C$, $D$, $E$, and $F$. The difference between them lies in the type of graph they represent. For example, in \Cref{fig:undirected_graph_def}, the edge C connects nodes 2 and 4. However, in \Cref{fig:direc_graph_def}, this connection provides additional information: a direction, which, in the context of this study, could represent the direction of a specific element, such as electric power or gas flow.


A graph can be represented in various ways using matrices, each capturing different aspects of the graph structure. The two most common matrix representations are the adjacency matrix and the incidence matrix.

The adjacency matrix of a graph is a square matrix used to represent the connections between vertices \cite{wilson_1972}. For a graph $G$ with $n$ vertices, the adjacency matrix $A$ is an $n \times n$ matrix where the entry $a_{ij}$ is defined as follows:

\begin{equation}
 a_{ij} = 
\begin{cases}
1 & \text{if there is an edge from vertex } $u$ \text{ to vertex } $v$, \\
0 & \text{otherwise}.
\end{cases}
    \label{eq:adjacency_matrix_definition}
\end{equation}


For a directed graph, the adjacency matrix captures the direction of the edges. Below is the adjacency matrix for the directed graph shown earlier:

\[
\mathbf{A} = \begin{pmatrix}
0 & 1 & 0 & 1 \\
1 & 0 & 0 & 1 \\
0 & 1 & 0 & 1 \\
1 & 1 & 1 & 0
\end{pmatrix}
\]


The incidence matrix of a graph represents the relationship between vertices and edges \cite{wilson_1972}. For a graph $G$ with $n$ vertices and $m$ edges, the incidence matrix $I$ is an $n \times m$ matrix where the entry $i_{ij}$ is defined as follows:

\begin{equation}
 b_{ij} =
\begin{cases}
1 & \text{if vertex } i \text{ is the starting point of edge } j \text{ in a directed graph}, \\
-1 & \text{if vertex } i \text{ is the endpoint of edge } j \text{ in a directed graph}, \\
0 & \text{if vertex } i \text{ is not connected to edge } j.
\end{cases}  
    \label{eq:incidence_matrix_definition}
\end{equation}


For the directed graph previously described, the incidence matrix is given by:

\[
\mathbf{B} = \begin{pmatrix}
1 & -1 & 0 & 0 \\
-1 & 0 & -1 & 0 \\
0 & 0 & 0 & 1  \\
0 & 1 & 1 & -1 
\end{pmatrix}
\]

\subsection{Multi-Layered Perceptrons}

A Multilayer Perceptron (MLP) is a fundamental type of artificial neural network, often regarded as one of the building blocks of deep learning. At its core, an MLP consists of multiple layers of nodes, or neurons, where each layer is fully connected to the next one. The architecture typically includes an input layer, one or more hidden layers, and an output layer, as can be seen in the \Cref{fig:FCNN_representation}.


\begin{figure}
    \centering
    \setlength\figurewidth{1\textwidth}        
    \setlength\figureheight{0.5\textwidth}
    \resizebox{\figurewidth}{\figureheight}{\input{figures/Chapter_LinealCensnet/MLP_def.tex}}
    % \input{figures/Chapter_LinealCensnet/MLP_def.tex}
    \caption{General diagram of a multilayer perceptron, showing the input layers in green, the hidden layers in blue and the outputs in red.}\label{fig:FCNN_representation}
\end{figure}



The neurons in each layer are connected to the neurons in the subsequent layer through weighted connections, the key parameters learned during the training process \cite{Hippert_Pedreira_Souza_2001}. One of the most significant properties of an MLP is its ability to work as a universal approximator. Given sufficient neurons in the hidden layers, an MLP can approximate any continuous function to an arbitrary degree of accuracy, provided the network is correctly trained \cite{Zhang_Eddy_Patuwo_Y_Hu_1998}.

Mathematically, an MLP can be defined as follows. Let $\mathbf{x} \in \mathbb{R}^n$ the input vector, where $n$ is the number of features. The output of each neuron in the first hidden layer is calculated as:

\begin{equation}
 \mathbf{z}^{(1)} = \sigma\left(\mathbf{W}^{(1)}\mathbf{x} + \mathbf{b}^{(1)}\right)   
    \label{eq:output_neuron}
\end{equation}



\noindent where $\mathbf{W}^{(1)} \in \mathbb{R}^{m_1 \times n}$ is the weighting matrix for the first hidden layer, with $m_1$ being the number of neurons in this layer, $\mathbf{b}^{(1)} \in \mathbb{R}^{m_1}$ is the bias vector, and $\sigma(\cdot)$ is the activation function, typically a non-linear function such as a ReLU (Rectified Linear Unit) or sigmoid function.

This process is repeated for each subsequent hidden layer $k$, where the output of the $k$-th layer is given by:

\begin{equation}
 \mathbf{z}^{(k)} = \sigma\left(\mathbf{W}^{(k)}\mathbf{z}^{(k-1)} + \mathbf{b}^{(k)}\right) \quad \forall i \in \{ 2, ..., L-1 \} 
    \label{eq:hidden_layers_output}
\end{equation}


Here, $\mathbf{W}^{(k)} \in \mathbb{R}^{m_k \times m_{k-1}}$ represents the weight matrix connecting layer $k-1$ to layer $k$, $\mathbf{b}^{(k)} \in \mathbb{R}^{m_k}$ is the bias vector for layer $k$, and $\mathbf{z}^{(k-1)}$ is the output of the previous layer.

Finally, the output layer produces the final prediction $\mathbf{\hat{y}}$:

\begin{equation}
    \mathbf{\hat{y}} = \sigma\left(\mathbf{W}^{(L)}\mathbf{z}^{(L-1)} + \mathbf{b}^{(L)}\right) 
    \label{eq:output_layer}
\end{equation}

\noindent where $L$ denotes the number of layers in the network, including the input and output layers. Depending on the nature of the problem (e.g., classification or regression), the activation function $\sigma(\cdot)$ used in the output layer can vary, with softmax being common in multi-class classification problems, and a linear activation for regression tasks. The entire MLP is trained using a process called backpropagation, combined with an optimization algorithm like gradient descent, to minimize a loss function $\mathcal{L}(\mathbf{y}, \mathbf{\hat{y}})$, which compares the true outputs $\mathbf{y}$ and the predicted outputs $\mathbf{\hat{y}}$. 


\subsection{Graph Neural Networks}

In recent years, Graph Neural Networks (GNNs) have emerged as a powerful tool in machine learning, particularly for tasks involving data that can be naturally represented as graphs. Graphs are a universal data structure that can model various systems in numerous fields, including social networks, biological networks, knowledge graphs, and physical systems. Because of their ability to represent relationships and interactions between entities, graphs are used extensively to model complex structures where the data points are not independent but interconnected \cite{Jia_Wang_Shou_Hosseini_Bai_2023}.

GNNs are important because they can directly operate on graph-structured data, extending neural networks' success from grid-like data structures, such as images and sequences, to more general and irregular structures \cite{Jia_Wang_Shou_Hosseini_Bai_2023a} . Traditional neural networks, like Convolutional Neural Networks (CNNs) or Recurrent Neural Networks (RNNs), are designed to work with data that has a fixed structure. However, many real-world problems involve data that can be better described by graphs, where nodes represent entities and edges represent relationships between those entities \cite{Gupta_Matta_Pant_2021}. 


Graph Neural Networks can be broadly defined as a class of neural networks designed to perform inference on data described by graphs. Formally, let \( G = (V, E) \) represent a graph, where \( V \) is the set of nodes (or vertices) and \( E \) is the set of edges. Each node \( v \in V \) can be associated with a feature vector \( \mathbf{x}_v \in \mathbb{R}^{D_v} \), where \( D_v \) is the number of features per node, and each edge \( (u, v) \in E \) may have an associated weighting or feature vector \( \mathbf{e}_{uv} \in \mathbb{R}^{D_e} \), where \( D_e \) is the number of features per edge. The goal of a GNN is to learn a representation for each node (or sometimes for the entire graph) by aggregating and transforming the feature information from the node's local neighborhood in the graph.


To move from the abstract concept to a practical implementation, the specific functions used for updating and aggregating node features must be defined \cite{Liu_Wu_Liu_Hu_2021}. In the GNN framework, message passing is understood as a series of iterations in which each node updates its representation by exchanging information with its neighbors. 

The basic message passing operation, which simplifies the original GNN model proposed by \cite{GRLB_Hamilton}, is expressed by the following equation:
\begin{equation}
 \mathbf{h}_u^{(k)} = \sigma\left( \mathbf{W}_{\text{self}}^{(k)} \mathbf{h}_u^{(k-1)} + \mathbf{W}_{\text{neigh}}^{(k)} \sum_{v \in \mathcal{N}(u)} \mathbf{h}_v^{(k-1)} + \mathbf{b}^{(k)} \right)  
    \label{eq:message_passing}
\end{equation}


\noindent In this equation:
\begin{itemize}
    \item \( \mathbf{h}_u^{(k)} \in \mathbb{R}^{D_h^{(k)}} \) represents the updated feature vector of node \( u \) at layer \( k \), where \( D_h^{(k)} \) is the dimensionality of the hidden representation at that layer.
    \item The term \( \mathbf{W}_{\text{self}}^{(k)} \mathbf{h}_u^{(k-1)} \), where \( \mathbf{W}_{\text{self}}^{(k)} \in \mathbb{R}^{D_h^{(k)} \times D_h^{(k-1)}} \) and \( \mathbf{h}_u^{(k-1)} \in \mathbb{R}^{D_h^{(k-1)}} \), transforms the node's own feature vector from the previous layer, enabling the node to retain and modify its own information. 
    \item The term \( \mathbf{W}_{\text{neigh}}^{(k)} \sum_{v \in \mathcal{N}(u)} \mathbf{h}_v^{(k-1)} \), where \( \mathbf{W}_{\text{neigh}}^{(k)} \in \mathbb{R}^{D_h^{(k)} \times D_h^{(k-1)}} \), aggregates the feature vectors of the neighboring nodes \( v \) in the set \( \mathcal{N}(u) \), and then applies a transformation via the weighting matrix. 
    \item \( \mathbf{b}^{(k)} \in \mathbb{R}^{D_h^{(k)}} \) is a bias term that can be added to the weighted sum, though it is sometimes omitted for simplicity. 
    \item The non-linear function \( \sigma(\cdot) \) is elementwise applied to introduce non-linearity into the model, which is essential for capturing complex patterns in the data.
\end{itemize}

In the context of Graph Neural Networks (GNNs), a Graph Convolutional Network (GCN) is a specialized model that applies the concept of convolution, widely used in image processing, to graphs. First introduced by \cite{Kipf:2016tc}, GCNs offer a method to perform deep learning on graph-structured data by extending traditional convolution operations to the irregular domain of graphs.

Specifically, the adjacency matrix \( \tilde{\mathbf{A}} \) is defined as:

\begin{equation}
 \tilde{\mathbf{A}} = \mathbf{A} + \mathbf{I}   
    \label{eq:modified_adjacency_matrix}
\end{equation}

\noindent with \( \mathbf{A} \) is the original adjacency matrix, and \( \mathbf{I} \) is the identity matrix. The identity matrix \( \mathbf{I} \) ensures that each node considers its own features when aggregating information from its neighbors.

The degree matrix \( \tilde{\mathbf{D}} \) is defined as:

\begin{equation}
    \tilde{D}_{vv} = \sum_{{v'} \in V} \tilde{A}_{vv^{'}}   
    \label{eq:degree_matrix}
\end{equation}
%
where \( V \) represents the set of all nodes in the graph. The diagonal entries of \( \tilde{\mathbf{D}} \) correspond to the degree of each node, adjusted to account for the added self-loops.




The fundamental idea behind GCNs is to create a spectral filter that operates on graph data. The filter's purpose is to combine features from a node's local neighborhood, taking into account the graph's structure. This process is mathematically formalized in the following way:

\begin{equation}
 \mathbf{H} = \sigma\left( \tilde{\mathbf{D}}^{-\frac{1}{2}} \tilde{\mathbf{A}} \tilde{\mathbf{D}}^{-\frac{1}{2}} \mathbf{X} \mathbf{\Theta} \right)   
    \label{eq:GCN_filter}
\end{equation}

\noindent Here:

\begin{itemize}
    \item \( \mathbf{H} \in \mathbb{R}^{N_v \times D_h} \) represents the matrix of node representations after applying the GCN layer. Each row \( \mathbf{h}_u \in \mathbb{R}^{D_h} \) in \( \mathbf{H} \) corresponds to the updated feature vector for node \( u \). 
    \item \( \mathbf{X} \in \mathbb{R}^{N_v \times D_v} \) is the matrix of input node features, where each row \( \mathbf{x}_u \in \mathbb{R}^{D_v} \) corresponds to the feature vector for node \( u \) before applying the GCN layer. 
    \item \( \sigma(\cdot) \) denotes a non-linear activation function elementwise applied to introduce non-linearity into the model. 
    \item \( \tilde{\mathbf{A}} \in \mathbb{R}^{N_v \times N_v} \) is the adjacency matrix of the graph, with added self-loops to account for the node itself in the aggregation. 
    \item \( \tilde{\mathbf{D}} \in \mathbb{R}^{N_v \times N_v} \) is the degree matrix of the graph, modified to include the self-loops. The degree matrix is diagonal, with each diagonal entry \( \tilde{D}_{ii} \in \mathbb{R} \) representing the degree of node \( i \) in the graph. 
    \item \( \mathbf{\Theta} \in \mathbb{R}^{D_v \times D_h} \) is a matrix of trainable parameters, which is learned during the training process to optimize the model's performance.
\end{itemize}


The expression \( \tilde{\mathbf{D}}^{-\frac{1}{2}} \tilde{\mathbf{A}} \tilde{\mathbf{D}}^{-\frac{1}{2}} \) is a normalized version of the adjacency matrix, ensuring that the eigenvalues of the operation are bounded between 0 and 1. This normalization step is crucial as it prevents issues such as exploding or vanishing gradients during the training of deep networks.

\subsection{Convolution with Edge-Node Switching (Cens Block)}

Graph Convolutional Networks (GCNs) have demonstrated considerable success in various graph-based machine learning tasks, particularly in their ability to generalize convolution operations to non-Euclidean data structures like graphs \cite{Fu_Wang_Liu_Liu_Zhou_You_Peng_Jing_2022} . GCNs operate by aggregating features from a node's neighbors, thereby capturing local neighborhood information and propagating it through the network layers. Despite their effectiveness, GCNs possess certain limitations that hinder their performance in more complex scenarios.



% One notable limitation of GCNs is their reliance solely on node features during the convolution process. This approach disregards the information contained within edge features \cite{8954414}. By neglecting edge features, GCNs fail to fully exploit the underlying graph structure, potentially missing out on critical insights that could enhance model performance \cite{8954414}.

One notable limitation of GCNs is their reliance on node features during the convolution process, which overlooks the information encoded in edge features. This limitation prevents the model from fully capturing the complexity of the graph structure and may reduce its overall effectiveness \cite{8954414}. Furthermore, GCNs typically aggregate information from immediate neighbors only, which can limit their ability to capture long-range dependencies in large or densely connected graphs. This restriction can lead to an oversimplified representation of the graph structure, particularly in cases where the graph contains intricate patterns that require deeper and more nuanced analysis \cite{Guo_Zhang_Teng_Lu_2019}.

To overcome the limitations of traditional GCNs, which focus primarily on node features, CensNet introduces a novel approach that integrates both node and edge features into the graph convolution process. The CensNet framework consists of two primary types of layers: the \textit{node layer} and the \textit{edge layer}. These layers work in concert to update node and edge embeddings alternately, leveraging the information from both nodes and edges in the graph \cite{9224195}.

The propagation rules in CensNet are designed to incorporate edge features into the convolution process, enabling a more comprehensive feature propagation across the graph. We define the normalized node adjacency matrix with self-loops as follows:

\begin{equation}
\tilde{\mathbf{A}}_v = \mathbf{D}_v^{-\frac{1}{2}} (\mathbf{A}_v + \mathbf{I}_{N_v}) \mathbf{D}_v^{-\frac{1}{2}},
    \label{eq:normalized_node_adjacency}
\end{equation}



\noindent In this expression \( \mathbf{A}_v \in \mathbb{R}^{N_v \times N_v} \) is the adjacency matrix for the nodes, and \( \mathbf{I}_{N_v} \in \mathbb{R}^{N_v \times N_v} \) is the identity matrix that introduces self-loops. The matrix \( \mathbf{D}_v \in \mathbb{R}^{N_v \times N_v} \) is the diagonal degree matrix of \( \mathbf{A}_v + \mathbf{I}_{N_v} \), where each diagonal entry corresponds to the degree of a node (i.e., the number of neighbors, including itself). The term \( N_v \) denotes the number of nodes in the graph. Consequently, the sum \( \mathbf{A}_v + \mathbf{I}_{N_v} \) is a matrix of size \( N_v \times N_v \), and so is \( \mathbf{D}_v \).

% \noindent where $\mathbf{D}_v$ is the diagonal degree matrix of $\mathbf{A}_v + \mathbf{I}_{N_v}$.

\paragraph{Node Layer Propagation:}


In the $(l+1)$-th layer, the node features are updated using the following propagation rule:


\begin{equation}
    \mathbf{H}^{(l+1)}_v = \sigma\left(\mathbf{T} \Phi\left(\mathbf{H}^{(l)}_e \mathbf{p}_e\right)\mathbf{T}^\top \odot \tilde{\mathbf{A}}_v \mathbf{H}^{(l)}_v \mathbf{W}_v\right) 
    \label{eq:CensNet_propagation}
\end{equation}

\noindent With, 

\begin{itemize}
    \item \( \mathbf{T} \in \{0, 1\}^{N_v \times N_e} \) is an incidence matrix that represents the connections between nodes and edges. Each element \( T_{v,e} \) indicates whether edge \( e \) connects to node \( v \). Specifically, if edge \( e \) is connected to node \( v \), then \( T_{v,e} = 1 \); otherwise, \( T_{v,e} = 0 \). Given that each edge is formed by two nodes, every column of the matrix \( \mathbf{T} \) will have exactly two elements equal to 1, corresponding to the two nodes that the edge connects. 

    \item \( \mathbf{H}^{(l)}_e \in \mathbb{R}^{N_e \times D_e} \) is the edge feature matrix from the \( l \)-th layer, where \( D_e \) is the dimensionality of the edge features. \( \mathbf{p}_e \in \mathbb{R}^{D_e} \) is a learnable vector of weights that projects the edge features into a scalar value per edge. The matrix-vector product \( \mathbf{H}^{(l)}_e \mathbf{p}_e \in \mathbb{R}^{N_e} \) results in a vector with one scalar value per edge. The operation \( \textup{diag}(\mathbf{H}^{(l)}_e \mathbf{p}_e) \in \mathbb{R}^{N_e \times N_e} \) converts this vector into a diagonal matrix where each element of the vector is placed on the diagonal.
   
       \item The Hadamard product, denoted by \( \odot \), represents element-wise multiplication between matrices. In this context, it combines the transformed edge features with the node adjacency matrix, integrating information from both the original graph and its line graph.
    \item \( \tilde{\mathbf{A}}_v \in \mathbb{R}^{N_v \times N_v} \) is the normalized adjacency matrix for nodes, as shown in \cref{eq:normalized_node_adjacency}, where \( \mathbf{A}_v \in \mathbb{R}^{N_v \times N_v} \) is the original node adjacency matrix and \( \mathbf{I}_{N_v} \in \mathbb{R}^{N_v \times N_v} \) is the identity matrix that introduces self-loops. This normalization ensures that the contributions from each node's neighbors are appropriately scaled. 

    \item \( \mathbf{H}^{(l)}_v \in \mathbb{R}^{N_v \times D_v} \) represents the node feature matrix from the \( l \)-th layer. \( \mathbf{W}_v \in \mathbb{R}^{D_v \times D_v'} \) is a learnable weight matrix that is applied to the node features during the propagation process.
    
    \item The activation function \( \sigma \) (typically a non-linear function such as ReLU) is applied element-wise to the resulting matrix to introduce non-linearity into the model.
\end{itemize}




This expression can be understood as a mechanism for integrating node and edge information. The matrix \( \mathbf{T} \) is responsible for transferring edge features into the node domain, allowing these edge-derived features to be merged with the normalized node adjacency matrix \( \tilde{\mathbf{A}}_v \). 


\paragraph{Edge Layer Propagation:}

Similarly, the normalized edge adjacency matrix is defined as:

\begin{equation}
    \tilde{\mathbf{A}}_e = \mathbf{D}_e^{-\frac{1}{2}} \left(\mathbf{A}_e + \mathbf{I}_{N_e}\right) \mathbf{D}_e^{-\frac{1}{2}},
    \label{eq:normalized_edge_adjacency}
\end{equation}


% \noindent where \( \mathbf{D}_e \) is the degree matrix corresponding to the edge adjacency matrix \( \mathbf{A}_e + \mathbf{I}_{N_e} \). The matrix \( \tilde{\mathbf{A}}_e \) serves as the normalized version of the edge adjacency matrix, similarly to the node adjacency normalization. This normalization ensures that the influence of each edge is scaled appropriately, which is crucial for the stability of the propagation process.


\noindent where \( \mathbf{D}_e \in \mathbb{R}^{N_e \times N_e} \) is the degree matrix corresponding to the edge adjacency matrix \( \mathbf{A}_e + \mathbf{I}_{N_e} \), with \( \mathbf{A}_e \in \mathbb{R}^{N_e \times N_e} \) and \( \mathbf{I}_{N_e} \in \mathbb{R}^{N_e \times N_e} \). The matrix \( \tilde{\mathbf{A}}_e \in \mathbb{R}^{N_e \times N_e} \) serves as the normalized version of the edge adjacency matrix, similarly to the node adjacency normalization. This normalization ensures that the influence of each edge is scaled appropriately, which is crucial for the stability of the propagation process.

The propagation rule for edge features is defined as follows:


\begin{equation}
    \mathbf{H}^{(l+1)}_e = \sigma\left(\mathbf{T}^\top \Phi\left(\mathbf{H}^{(l)}_v \mathbf{P}_v\right) \mathbf{T} \odot \tilde{\mathbf{A}}_e \mathbf{H}^{(l)}_e \mathbf{W}_e\right),
\end{equation}


In this expression, the following components are involved:




\begin{itemize}
    \item \textbf{\( \mathbf{T}^\top \in \{0,1\}^{N_e \times N_v} \)} is the transpose of the binary transformation matrix \( \mathbf{T} \in \{0,1\}^{N_v \times N_e} \) used in the node layer propagation. The matrix \( \mathbf{T}^\top \) maps the node features back into the edge domain, allowing the edge features to be updated based on the node information.
    
    \item \( \mathbf{H}^{(l)}_v \in \mathbb{R}^{N_v \times D_v} \) is the node feature matrix from the \( l \)-th layer, and \( \mathbf{p}_v \in \mathbb{R}^{D_v} \) is a learnable vector that projects each node's features to a scalar. The operation \( \mathbf{H}^{(l)}_v \mathbf{p}_v \in \mathbb{R}^{N_v} \) results in a scalar per node. The function \( \Phi(\cdot) \) produces the diagonal matrix \( \Phi\left(\mathbf{H}^{(l)}_v \mathbf{p}_v\right) \in \mathbb{R}^{N_v \times N_v} \), which is used to scale each node's influence in the projection.
    
    \item The matrix product \( \mathbf{T}^\top \Phi\left(\mathbf{H}^{(l)}_v \mathbf{p}_v\right) \mathbf{T} \in \mathbb{R}^{N_e \times N_e} \) maps this node-level scalar information into the edge domain. This matrix has the same dimensions as the normalized edge adjacency matrix \( \tilde{\mathbf{A}}_e \in \mathbb{R}^{N_e \times N_e} \), which ensures the Hadamard product is dimensionally valid.
    
    \item \( \mathbf{H}^{(l)}_e \in \mathbb{R}^{N_e \times D_e} \) represents the edge feature matrix from the \( l \)-th layer, while \( \mathbf{W}_e \in \mathbb{R}^{D_e \times D_e'} \) is a learnable weight matrix. Their product \( \mathbf{H}^{(l)}_e \mathbf{W}_e \in \mathbb{R}^{N_e \times D_e'} \) results in the updated edge features before aggregation, where \( D_e' \) denotes the dimensionality of the edge features in the \( l+1 \)-th layer.
    
    \item The Hadamard product \( \odot \) element-wise multiplies the matrix \( \mathbf{T}^\top \Phi\left(\mathbf{H}^{(l)}_v \mathbf{p}_v\right) \mathbf{T} \in \mathbb{R}^{N_e \times N_e} \) with \( \tilde{\mathbf{A}}_e \in \mathbb{R}^{N_e \times N_e} \), producing a filtered adjacency matrix in the edge domain.
    
    \item The resulting matrix (after Hadamard product) multiplies the transformed edge features \( \mathbf{H}^{(l)}_e \mathbf{W}_e \in \mathbb{R}^{N_e \times D_e'} \), resulting in an output matrix in \( \mathbb{R}^{N_e \times D_e'} \), consistent with the desired shape of \( \mathbf{H}^{(l+1)}_e \).
    
    \item As in the node layer propagation, the activation function \( \sigma \) is applied element-wise to introduce non-linearity.
\end{itemize}

This propagation rule updates the edge embeddings by integrating information from the node features and the edge structure, thereby enhancing the expressiveness of the edge representations. The alternating updates between node and edge embeddings allow the model to effectively bridge signals across nodes and edges, resulting in more informative graph embeddings compared to other neural network approaches.




\subsection{Gas flow estimation through CensNet} \label{sec:LinealCensnet_formulation}

The natural gas system consists of a network of production centers, pipelines, compressor stations, storage facilities, and distribution points that ensure reliable gas delivery from producers to consumers. Mathematically, this system can be represented as a directed graph defined as $ G = (V, E)$ where $V$ is the set of units within the gas system, and $E$ is the set of different elements linking them. This set of units includes gas supply nodes or wells $\mathcal{W} \subset V$, gas demand nodes or users $\mathcal{U} \subset V$, and gas storage facilities $\mathcal{S} \subset V$. Similarly, the set of directed gas adjacency edges $\mathcal{A} = \left\{(n,m) \mid n,m\in V \right\} \subset E$ delineates the network structure through two kinds of transmission elements: transport pipelines $\mathcal{P} = \left\{p=(n,m) \mid n,m\in V \right\}$ and compressing stations $\mathcal{C} = \left\{c=(n,m) \mid n,m \in V \right\}$, so that $\mathcal{P}\cup\mathcal{C}=\mathcal{A}$ and $\mathcal{P}\cap\mathcal{C}=\emptyset$.

Natural gas transportation requires coordination to manage the flow through the different elements to maintain safe operating ranges. In optimizing this network, mathematical models minimize overall operating costs associated with the various stages of natural gas transportation, compression, storage, and handling unsupplied demand, ensuring compliance with technical and physical constraints. The cost function to minimize is expressed as:

\begin{equation} \label{eq:obj_func_linear_gas}
    \begin{split}
        \min_{\mathcal{P}, \mathcal{F}} \quad \sum_{t \in \mathcal{T}} \sum_{w \in \mathcal{W}} C_{w}^t {f_{w}^t} + \sum_{p \in \mathcal{P}} C_{p}^t {f_{p}^t} + \sum_{c \in \mathcal{C}} C_{c}^t {f_{c}^t} + \sum_{u \in \mathcal{U}} C_{u}^{t} {f_{u}^{t}} 
        % + \quad \sum_{s \in \mathcal{S}} C_{s+}^{t} {f_{s+}^{t}}  + \sum_{s \in \mathcal{S}} C_{s-}^{t} {f_{s-}^{t}} + \sum_{s \in \mathcal{S}} C_{s}^{t} {V_{s}^{t}}
    \end{split}
\end{equation}


Here, $\mathcal{T}$ denotes the set of discrete time periods considered in the optimization model. The term $\sum_{w \in \mathcal{W}} C_{w}^t {f_{w}^t}$ represents the total cost of gas production at the wells, where $C_{w}^t$ denotes the cost per unit flow of gas at a specific well $w$ during time period $t$, and $f_{w}^t$ corresponds to the flow of gas from well $w$. Similarly, the transportation of gas through pipelines is captured by the term $\sum_{p \in \mathcal{P}} C_{p}^t {f_{p}^t}$, where $C_{p}^t$ is the cost per unit flow through pipeline $p$ during time period $t$, and $f_{p}^t$ represents the flow of gas through pipeline $p$. In addition, the total cost associated with gas compression at compressor stations is accounted for by $\sum_{c \in \mathcal{C}} C_{c}^t {f_{c}^t}$, where $C_{c}^t$ is the cost per unit flow at compressor station $c$ during time period $t$, and $f_{c}^t$ is the flow of gas through compressor station $c$. Beyond production, transportation, and compression, the model also considers the costs related to unmet gas demand. The term $\sum_{u \in \mathcal{U}} C_{u}^{t} {f_{u}^{t}}$ reflects the penalty cost associated with unsupplied gas demand, where $C_{u}^{t}$ is the penalty cost per unit of unsupplied gas at location $u$ during time period $t$, and $f_{u}^{t}$ represents the volume of unmet demand. 

    The constraint set models the gas architecture of the transportation system and its technical limits: \Cref{eq:well_limits} forces each production well to inject the flow $f_{w}^t \in \mathbb{R}^{N_v} $ over the technical minimum $\underline{f_{w}^t}  \in \mathbb{R}^{N_v}$ and under the maximum capacity $\overline{f_{w}^t} \in \mathbb{R}^{N_v}$. \Cref{eq:pipe_limits} upper-bounds the gas flow through pipelines $f_{p}^t \in \mathbb{R}^{N_e} $ to the structural capacity $\overline{f_{p}^t} \in \mathbb{R}^{N_e} $. \Cref{eq:dem_limit_gas} ensures that the unsupplied demand $f_{u}^{t}\in \mathbb{R}^{N_v}$ is lower than the corresponding user demand $\overline{f_{u}^{t}}\in \mathbb{R}^{N_v}$. The nodal gas balance in \Cref{eq:gas_balance} guarantees that the gas entering the node $n$ equals the gas leaving it. 

\begin{alignat}{4}
    \underline{f_{w}^t} \leq f_{w}^t \leq \overline{f_{w}^t} &\quad \forall \ w \in \mathcal{W} \label{eq:well_limits} \\
    -\overline{f_{p}^t} \leq f_{p}^t \leq \overline{f_{p}^t} &\quad \forall \ p \in \mathcal{P} \label{eq:pipe_limits} \\
    % \underline{\pi_{n}^t} \leq \pi_{n}^t \leq \overline{\pi_{n}^t} &\quad \forall \ n \in \mathcal{N}_f \label{eq:press_limit} \\
    % \pi_{m}^t \leq \beta_{c}^t{\pi_{n}^t} &\quad \forall c=(n,m) \in \mathcal{C} \label{eq:comp_ratio} \\
    0 \leq f_{u}^{t} \leq \overline{f_{u}^{t}} &\quad \forall \ u \in \mathcal{U} \label{eq:dem_limit_gas} \\
    \sum_{m:(m,n)\in\mathcal{A}}{f_{m}^t} = \sum_{m':(n,m')\in\mathcal{A}}{f_{m'}^t} &\quad \forall \ n \in \mathcal{N}_f \label{eq:gas_balance}
    % 0 \leq f_{s+}^t \leq V_{0s} - \underline{V_s} &\quad \forall \ s \in \mathcal{S} \label{eq:sto_limit1} \\ 
    % 0 \leq f_{s-}^t \leq \overline{V_s} - V_{0s} &\quad \forall \ s \in \mathcal{S} \label{eq:sto_limit2} \\ 
    % V_{s}^t = V_{s}^{t-1} + f_{s-}^{t-1} - f_{s+}^{t-1} &\quad \forall \ s \in \mathcal{S} \label{eq:sto_time}\\
    % sgn(f_{p}^t)(f_{p}^t)^2 = K_{nm}((\pi_{n}^t)^2-(\pi_{m}^t)^2) &\quad \forall \ p =(n,m) \in\mathcal{P} \label{eq:weymouth_cons}
\end{alignat}

The optimization model described above is used to generate flow scenarios across the network. These scenarios serve as supervised training data for a CensNet-based model that approximates the mapping between the structural and technical parameters of the network and the resulting gas flows. The CensNet architecture is designed to learn these mappings by taking as input a set of graph-based features and predicting the flow behavior directly through a regression task. Specifically, the network receives the following input data:


\begin{itemize}
    \item \textbf{Node Features \(\mathbf{X} \in \mathbb{R}^{N_v \times 3}\):} A matrix where each row corresponds to a node in the graph, and each column represents a node-level attribute. Each node feature vector includes the lower and upper limits for injected flow, as well as demanded flow.

    \item \textbf{Node Laplacian \(\mathbf{L}_v \in \mathbb{R}^{N_v \times N_v}\):} An adjacency-based Laplacian matrix encoding the graph structure of the nodes. This matrix captures the relational topology among nodes and is used in spectral formulations of GNNs.

    \item \textbf{Edge Laplacian \(\mathbf{L}_e \in \mathbb{R}^{N_e \times N_e}\):} A Laplacian matrix defined over the edges, capturing connectivity patterns between edges, relevant in edge-based message passing formulations.

    \item \textbf{Incidence Matrix \(\mathbf{T} \in \{0,1\}^{N_v \times N_e}\):} A binary matrix representing the node-edge incidence relationship. Each column corresponds to an edge and contains ones in the rows of its incident nodes, enabling the mapping of flows through the network.

    \item \textbf{Edge Features \(\mathbf{E} \in \mathbb{R}^{N_e \times 5}\):} A matrix where each row corresponds to an edge in the graph, and each column represents an edge-level attribute. Each edge feature includes the \(K\) constant, the maximum compression ratio \(\beta\), and the upper and lower flow limits.
\end{itemize}


The model outputs include node-level predictions corresponding to the injected flow at each node and edge-level predictions representing the transported flow across the network edges.

\subsection{Task-Dependent Loss Functions}

The output layer and corresponding loss functions in CensNet are designed to be task-dependent. For regression tasks, the loss function can be formalized as a regularized mean square error (MSE) loss. The MSE loss measures the difference between the predicted outcomes and the actual values, providing a natural fit for regression problems.


We define the loss function for graph regression as follows:

\begin{equation}
    \mathcal{L}(\Theta) = \sum_{r=1}^{R} \| Y_r - \hat{Y}_r \|^2_2 + \lambda \|\Theta\|_p,
\end{equation}

\noindent where:

\begin{itemize}
    \item \( Y_r \in \mathbb{R} \) is the ground-truth value for the \( r \)-th regression target.
    \item \( \hat{Y}_r \in \mathbb{R} \) is the predicted value for the same feature, computed from the final node representation of the graph.
    \item \( \| Y_f - \hat{Y}_r \|^2_2 \) is the squared error for feature \( r \), and the sum is taken over all \( R \) output features.
    \item \( \Theta \in \mathbb{R}^{P} \) is the set of learnable parameters in the model, and \( \|\Theta\|_p \) denotes the \( L_p \) norm regularization term.
    \item \( \lambda \in \mathbb{R}_+ \) is a regularization coefficient that penalizes overly complex models to reduce overfitting.
\end{itemize}




\section{Experimental Setup and Results} \label{sec:LinealCensnet_ExperimentalSetup}

The generated samples served as training data for a CensNet-based model to solve the natural gas transportation problem, which was designed as a faster alternative to the optimization-based model. This model is built to focus on predicting node and edge-level characteristics, incorporating the structure of the network and its connectivity into the learning process. 

In this experimental setup, different network operation scenarios were generated by perturbing the users' consumption values with added noise and subsequently solving each scenario using the linear constrained optimization model presented in the previous section. The optimization was performed using APOPT (Advanced Process OPTimizer) through the Python package GEKKO \cite{Beal_Hill_Martin_Hedengren_2018}, a solver for large-scale linear and nonlinear optimization problems. APOPT was used to obtain the output variables for each scenario, serving as the ground truth data with which the CensNet model was trained. The noise levels ranged from 5\% to 25\%, applied to the parameters of the networks to simulate diverse operating conditions. 


The first network is a small-scale test network consisting of 8 nodes, while the second represents the Colombian natural gas transportation system, a more extensive and complex network consisting of 63 nodes. These networks were used to evaluate the performance of the proposed model under varying scenarios. The model penalizes deviations in node and edge losses, which directly impact gas flow efficiency through the system. To achieve this, the architecture is structured as a CensNet-based neural networks, with customizable depth (number of layers), channels, and dense layers, ensuring flexibility in adapting to small-scale and large-scale networks, such as the Colombian system.

A general outline of the model can be seen in \cref{fig:model_description} and the basic components of the model are explained below:

\begin{itemize}
        
    \item \textbf{Normalization and Pre\-dense Layers:} The node and edge inputs receive feature-wise normalization to standardize the data. Following this, the inputs are passed through two dense layers, each with $N$ channels. The purpose of these pre-dense layers is to transform the feature space before applying the convolutional layers, enabling the network to project the raw input data into a higher-dimensional latent space that facilitates more expressive and effective learning in the subsequent convolutional stages.


    \item \textbf{CensNet Blocks Layers:} The main body of the network consists of \(N\) convolutional blocks. Each block applies a CensNet convolution, which updates both node and edge features by considering the structural relationships encoded in the node and edge Laplacians, as well as the incidence matrix. Batch normalization follows each convolution to stabilize learning. This structure allows the model to capture complex interactions between nodes and edges and propagate information across the graph, learning how local features influence the broader system.
   
    \item \textbf{Post-dense Layer:} After passing through the convolutional blocks, the node and edge features are further processed by a series of dense layers. The number of dense layers $N \ dense$ is adjustable. These layers further refine the learned features, enabling the model to output node and edge-level predictions. 
   
    
    \item \textbf{Losses and Outputs:} The final outputs of the network are the node-level and edge-level predictions, denoted by \(\hat{\mathbf{X}}_v\) and \(\hat{\mathbf{X}}_e\), respectively. The node predictions \(\hat{\mathbf{X}}_v\) correspond to the estimated flow at each node, while the edge predictions \(\hat{\mathbf{X}}_e\) represent the flow along the edges. Both outputs are penalized based on their respective losses, which are calculated by comparing the predicted values to ground truth values and evaluating how well the physical constraints are respected.


The loss functions ensure that the model accurately predicts the node and edge flows while satisfying the physical constraints of the system. These constraints are essential for ensuring that the predicted flows are feasible within the operational limitations of the network. 
    \item \textbf{Model Optimization:} The model is trained using backpropagation with the Adam optimizer. The training process involves minimizing the node and edge loss functions, which penalize incorrect flow predictions and deviations from the expected behavior of the network.

\begin{figure}
    \centering
    \setlength\figurewidth{1\textwidth}        
    \setlength\figureheight{0.4\textwidth}
    \resizebox{\figurewidth}{\figureheight}{\input{figures/Chapter_LinealCensnet/model_description.tex}}
    % \input{figures/Chapter_LinealCensnet/MLP_def.tex}
    \caption{General outline of the CensNet-based model used.}
    \label{fig:model_description}
\end{figure}
\end{itemize}


The network training was done under the following conditions: data was partitioned into training, validation, and test sets, with percentages of 60\%, 20\%, and 20\% using 2000 samples for the 8-node network and 2400 for the 63-node network. The learning rate schedule followed an Exponential Decay approach, with an initial learning rate of $1 \times 10^{-2}$ decay steps of 1000, and a decay rate of 0.9. The model was trained over 1500 epochs, ensuring the parameters had sufficient time to converge for both node- and edge-level predictions using a Leaky Relu activation function with an alpha parameter of 0.2.


The model utilized three hyperparameters: $N \ channels$, which corresponds to the units in the pre-dense and convolutional layers (CensNet), $N \ dense$, representing the number of post-dense layers, and $N \ layers$, denoting the number of convolutional layers. Additionally, the model penalizes deviations in the network's response, specifically targeting MSE between the actual flow and the predicted flow at both the nodes and pipelines. Two tests were performed to determine the optimal set of hyperparameters. The objective was to compare the effect of using different combinations of loss functions during training. In the first test, only the weight associated with the nodal flow loss was considered, while in the second test, both nodal and edge flow losses were included. A test using only the edge loss was not conducted, as it was considered inappropriate to disregard the nodal flow loss. This is because the nodal flows represent the injection and demand values, which are essential for the physical consistency of the system and are directly associated with the behavior of the network at each node. In each test, the hyperparameters were optimized using the open-source Optuna framework \cite{optuna_2019}, with the following search space: $N \ channels$ ranged from 16 to 64, $N \ layers$ from 1 to 5, and $N \ dense$ from 2 to 32.


% Two tests were performed to determine the optimal set of hyperparameters. The objective was to compare the effect of using different combinations of loss functions during training. In the first test, only the weight associated with the nodal flow loss was considered, while in the second test, both nodal and edge flow losses were included. A test using only the edge loss was not conducted, as it was considered inappropriate to disregard the nodal flow loss. This is because the nodal flows represent the injection and demand values, which are essential for the physical consistency of the system and are directly associated with the behavior of the network at each node.


% \subsection{Results}
%
% In this section, we present the results of the proposed GNN model, focusing on the relationship between the predicted outputs and the actual observed values in the natural gas transportation networks. The evaluation includes both the 8-node test network and the Colombian natural gas system, with the goal of assessing the model's ability to predict key parameters under varying operational conditions. 


%
% \begin{figure}
%     \centering
%     \input{figures/Chapter_LinealCensnet/yn_dummy_base.tex}
%     \caption{Your caption here}\label{fig:your-label-here}
% \end{figure}
%

% \subsection{Case Study I: 8-node Network}
%
%
% As a result of the optimization process for this first test, the following hyperparameters were obtained: $N \ channels=21$, $N \ layers=5$, and $N \ dense=4$. With these optimized values, the model achieved an MSE of 1.98. In \Cref{fig:results_dummy_base_node}, a scatter plot illustrates the relationship between the actual values of gas generation at the nodes and the corresponding values predicted by the trained neural network, considering only the losses at the nodes. The plot highlights how effectively the network captures the fact that only one of the nodes in the system has gas generation, specifically the one with the associated gas injection field. However, while the network successfully identifies the generating node, the predicted values exhibit less dispersion than the actual value, indicating that the model's predictions are more concentrated around a certain value.
%
%
% \Cref{fig:results_dummy_base_f} displays the relationship between the actual gas flows through the edges and the predictions made by the neural network. In this case, the model struggled to predict the flow values accurately, demonstrating a significant deviation from the actual data. This result is not unexpected, as the model used in this experiment focused solely on the losses related to gas flows at the injection nodes without accounting for the gas transported thvalue the edges. As such, the absence of edge-related loss consideration likely contributed to the poor prediction performance for edge flows.
%
%
% \begin{figure}
%     \centering
%     \setlength\figurewidth{.53\textwidth}        
%     \setlength\figureheight{0.36\textwidth} 
%     \subfloat[Nodal flows.] 
%     {\label{fig:results_dummy_base_node}\resizebox{\figurewidth}{\figureheight}{\input{figures/Chapter_LinealCensnet/yn_dummy_base.tex}}}
%     \subfloat[Edge flows.] 
%     {\label{fig:results_dummy_base_f}\resizebox{\figurewidth}{\figureheight}{\input{figures/Chapter_LinealCensnet/yn_dummy_f.tex}}}
%     
%     \caption{Model results using only the loss associated with nodal flow predictions in the 8-node network.}
%     \label{fig:dummy_base_results}
% \end{figure}
%
%
%
% To compare the computational times of the natural gas linear optimization model and the GNN-based model, a t-test was performed, comparing the time each took to generate 100 predictions for new, unseen cases. The results showed a T-statistic of $14.94$, 198 degrees of freedom, and a p-value of 1.32e-34, indicating that the optimization model took significantly longer than the GNN-based model, confirming its higher computational cost.
%
% In the next stage of the experiment, the model was updated to account for the loss associated with edge flow predictions. This allowed the network to better capture the dynamics of gas transportation across the entire network, not just at the nodes. In this case, the hyperparameter tuning revealed that the best combination was $N \ channels=44$, $N \ layers=4$, and $N \ dense=4$, with a test data loss of 2.02, where 1.996 corresponds to the loss associated with the prediction of flows injected at the nodes, and 0.207 corresponds to the loss related to the prediction of flows through the edges. Additionally, the MSE between the gas balance based on actual flows and predicted flows was calculated, resulting in a loss of 1.719. It is essential to highlight that, in the previous experiment, this same loss was 284.764. This loss was computed outside the training stage, so it did not influence the optimization of parameters or hyperparameters.
%
% \Cref{fig:results_dummy_node_base_f} shows the relationship between the actual injection values at the nodes and the corresponding predictions. As in the previous experiment, the model successfully recognizes that gas injection occurs only at a single node, although the predicted values differ slightly from the actual data. However, a significant improvement is observed when analyzing the flows through the edges. \Cref{fig:results_dummy_edge_base_f} highlights this, where the network's predictions for edge flows are almost perfectly aligned with the actual test data, as evidenced by an $R^2$ of 0.999. The near-perfect scatter plot in \Cref{fig:results_dummy_edge_base_f} demonstrates the network's ability to make highly accurate predictions for gas flows through the edges in this updated experiment.
%
% \begin{figure}
%     \centering
%         \setlength\figurewidth{.53\textwidth}        
%         \setlength\figureheight{0.36\textwidth} 
%         \subfloat [Nodal flows.] {\label{fig:results_dummy_node_base_f}\resizebox{\figurewidth}{\figureheight}{\input{figures/Chapter_LinealCensnet/yn_dummy_base_f.tex}}}
%         \subfloat [Edge flows.] {\label{fig:results_dummy_edge_base_f}\resizebox{\figurewidth}{\figureheight}{\input{figures/Chapter_LinealCensnet/ye_dummy_base_f.tex}}}
%         \caption{Model results using the losses associated with the flows in nodes and edges predictions in the 8-node network.}
%         \label{fig:dummy_base_f_results}
% \end{figure}
%
%
% To evaluate the computational performance when considering the loss associated with pipelines and compressor flows, a t-test was performed comparing the computation times of the optimization model and the GNN-based model. The results showed a T-statistic of $14.81$, $198$ degrees of freedom and a p-value of $3.47e-34$, indicating that the optimization model took significantly longer to compute than the GNN-based model. This confirms the consistently higher computational cost of the optimization model, even when additional complexities, such as pipeline flows, are included in the network.
%
% \begin{table}[H]
%     \centering
%     \begin{tabular}{|l|c|c|c|c|}
%         \hline
%         Method & Node Loss & Edge Loss & Balance Loss & Time \\ \hline
%         APOPT & 4.84  12.87 & 24.22  14.04 & 0  0.04 & 0.79  0.44 \\ \hline
%         GNN (N) & 4.84  12.79 & 0.46  0.26 & -5.71  16.85 & 0.14  0.04 \\ \hline
%         GNN (N+E) & 4.77  12.7 & 24.14  14.08 & -0.08  1.17 & 0.15  0.06 \\ \hline
%     \end{tabular}
%     \caption{Comparison of mean and standard deviation values for nodal flows, edge flows, nodal balance, and prediction time between the optimization model and the GNN-based model, over 100 samples. The notation \textbf{GNN (N)} refers to experiments considering only the nodal loss, and \textbf{GNN (N+E)} to experiments considering both nodal and edge losses.}
%     \label{tab:lineal_dummy_results}
% \end{table}
%
% \Cref{fig:results_dummy_node_base_f} illustrates the relationship between the actual gas injection values at the nodes and the corresponding predictions. As in the previous experiment, the mode identifies that gas injection occurs at only one node, although the predicted values exhibit minor deviations from the actual data. An improvement is observed when analyzing the flows along the edges. \Cref{fig:results_dummy_edge_base_f} demonstrates this, showing that the networks edge flow predictions are almost aligned with the actual test values, as reflected by an $R^2$ score of 0.999. 
%
%
% The \Cref{tab:lineal_dummy_results} presents a combined comparison of the optimization model (APOPT) and the GNN-based model across four experiments. Each experiment has three degrees of freedom: the model method (APOPT or GNN), whether nodal loss (N) was included, and whether edge loss (E) was included. The table focuses on four key metrics: nodal flows, edge flows, nodal balance, and prediction time. Mean and standard deviation values are reported based on 100 samples.
%
% Both models display similar results for nodal flows in the experiments considering only the nodal loss. The GNN shows a slightly lower mean when both nodal and edge losses are considered, maintaining performance close to that of the APOPT model. On the other hand, edge flow values vary more significantly between the models. In experiments with only nodal loss, the GNN-based model achieves much lower values compared to the APOPT model. However, when both losses are included, the GNN model closely matches the performance of the optimization model, demonstrating its ability to replicate edge flow predictions under these conditions.
%
% Regarding nodal balance, the APOPT model remains consistent across all experiments, showing a near-zero mean. The GNN-based model fluctuates more, especially in the nodal-only experiment, though this variance is significantly reduced when both losses are considered. Finally, the GNN model consistently outperforms APOPT in terms of prediction time. Across all experiments, the GNN-based model achieves much faster predictions, making it particularly appealing for real-time applications.
%
%
% \subsection{Case Study II: 63-node Network (Colombia)}
%
%
% The second studied case corresponds to a gas transportation network consisting of 63 nodes representing the Colombian natural gas system. In the first test, where only the loss associated with the flows injected into the nodes was considered the hyperparameter tuning resulted in the following values: $N \ channels = 43$, $N \ layers = 2$, and $N \ dense = 2$. The total loss obtained was 10.598, with the loss associated with the nodal flows. The gas balance loss was 3426.659, but it is important to note that only the nodal flows were considered during the network optimization, and the balance loss was computed after the training process.
%
% The \Cref{fig:yn_col_base} shows the scatter plot relating the actual injected flows at the nodes to the predicted flows. This figure demonstrates the network's ability to accurately identify which nodes had gas injections and which did not. Furthermore, with an $R^2$ value of 0.996, the network effectively captured the correct proportions of these injected flows, indicating a robust predictive performance. On the other hand, \Cref{fig:ye_col_base} presents the relationship between the actual and predicted flows through the edges, encompassing both gas pipelines and compressors. In this case, the network struggled to predict the flow values accurately for these elements, which is unsurprising given that the loss associated with the edge flows was not included in the network's optimization process for this test.
%
% \begin{figure}
%     \centering
%         \setlength\figurewidth{.53\textwidth}        
%         \setlength\figureheight{0.36\textwidth} 
%         \subfloat [Nodal flows.] {\label{fig:yn_col_base}\resizebox{\figurewidth}{\figureheight}{\input{figures/Chapter_LinealCensnet/yn_col_base.tex}}}
%         \subfloat [Edge flows.] {\label{fig:ye_col_base}\resizebox{\figurewidth}{\figureheight}{\input{figures/Chapter_LinealCensnet/ye_col_base.tex}}} 
%     \caption{Model results using only the loss associated with nodal flow predictions in the colombian 63-node network.}
%     \label{fig:lineal_col_base_results}
% \end{figure}
%
%
% For the current experiment stage, involving the 63-node Colombian network, a t-test was conducted to compare the computation times between the optimization model and the GNN-based model. The test yielded a T-statistic of $47.29$, 198 degrees of freedom and a p-value of $4.92 \times 10^{-110}$, demonstrating a highly significant difference in the computational times. As with the previous experiments, these results confirm that the GNN-based model significantly outperformed the optimization model in terms of prediction speed.
%
%
% In the next stage of this case study, predictions were made on the same system, but this time, considering not only the losses associated with the flows injected at the nodes but also those corresponding to the flows transported by the system's pipelines and compressors. For this experiment, the hyperparameter tuning resulted in $N \ channels = 25$, $N \ layers = 5$, and $N \ dense = 4$, with a total loss of 34.4. Of this, 10.8 were associated with the predictions of the flows injected at the nodes, while 23,6 corresponded to the flows transported through the edges. The MSE for the gas balance was 279.8.
%
% \Cref{fig:yn_col_base_f} illustrates that, as in the previous case, the network can predict which nodes can inject natural gas and demonstrate a certain degree of accuracy regarding their injection capacity. The critical difference in this test, compared to the previous one, is shown in \cref{fig:ye_col_base_f}. Here, it is evident that by including the losses associated with the edges of the Colombian transportation network, the model significantly improves its predictions for these elements, achieving an $R^2$ value of 0.996 compared to the actual data.
%
%
% \begin{figure}
%     \centering
%         \setlength\figurewidth{.53\textwidth}        
%         \setlength\figureheight{0.36\textwidth} 
%         \subfloat [Actual vs predicted nodal flows.] {\label{fig:yn_col_base_f}\resizebox{\figurewidth}{\figureheight}{\input{figures/Chapter_LinealCensnet/yn_col_base_f.tex}}}
%         \subfloat [Actual vs predicted edge flows.] {\label{fig:ye_col_base_f}\resizebox{\figurewidth}{\figureheight}{\input{figures/Chapter_LinealCensnet/ye_col_base_f.tex}}}
%         % \includegraphics[width=0.45\textwidth]{figures/}
%         \caption{Model results using the losses associated with the predicted flows injected at the nodes and transported by the pipelines and compressors in the colombian 63-node network.}
%         \label{fig:col_base_f_results}
% \end{figure}
%
%
% In this stage, a t-test was performed to compare the computation times between the optimization model and the GNN-based model. The results yielded a T-statistic of $47.25$, $198$ degrees of freedom and a p-value of $4.46 \times 10^{-110}$. These values confirm with high statistical confidence that the GNN-based model significantly outperforms the optimization model in terms of computational efficiency when predicting flows across the network.
%  
%  
% \begin{table}[htbp]
% \centering
% \begin{tabular}{|l|c|c|c|c|}
%     \hline
%     Method & Node Loss & Edge Loss & Balance Loss & Time \\ \hline
%     APOPT & 11.41  49.23 & 63.52  81.62 & -2.15  16.49 & 5.01  5.59 \\ \hline
%     GNN (N) & 11.38  49.13 & 0.91  1.26 & -2.19  58.56 & 0.13  0.07 \\ \hline
%     GNN (N+E) & 11.40  48.97 & 63.45  81.38 & -2.16  16.60 & 0.14  0.08 \\ \hline
% \end{tabular}
% \caption{Comparison of mean and standard deviation values for nodal flows, edge flows, nodal balance, and prediction time between the optimization model (APOPT) and the GNN-based model, across 100 samples. The notation \textbf{GNN (N)} refers to experiments considering only the nodal loss, and \textbf{GNN (N+E)} to experiments considering both nodal and edge losses.}
% \label{tab:lineal_col_results}
% \end{table}
%
% The \Cref{tab:lineal_col_results} presents a comparison between the optimization model (APOPT) and the GNN-based model across four experiments, which vary based on the method used (APOPT or GNN) and the inclusion of nodal loss (N) and edge loss (E). The comparison focuses on nodal flows, edge flows, nodal balance, and prediction time, with the mean and standard deviation calculated from 100 test samples.
%
% In terms of nodal flows, the GNN-based model achieves results that closely resemble those of the optimization model across all experiments. The consistency in both the mean values and standard deviations indicates that the GNN can approximate the nodal flow patterns learned from the optimizer, regardless of whether it was trained using only the nodal loss or both nodal and edge losses.
%
% For edge flows, the differences between models are more pronounced. When the GNN is trained without considering edge loss, its edge flow predictions diverge significantly from those of the optimization model. However, once edge loss is incorporated into the training process, the GNN's edge flow outputs become much more aligned with the optimizer's results in both mean and variability. This indicates that including edge loss is essential for accurately reproducing the edge flow patterns generated by the optimization model.
%
% Regarding nodal balance, all models yield results with similar means. Nevertheless, the GNN exhibits higher variability when trained with nodal loss alone. This variability is reduced when edge loss is included, leading to balance predictions that not only remain close to those of the optimizer but also exhibit comparable consistency. Since balance values closer to zero are preferred, the inclusion of edge loss improves the quality of the GNNs output in this respect.
%
% Finally, the GNN-based model significantly outperforms the optimization model in terms of computational efficiency. Prediction times are consistently lower for the GNN across all experiments, with only a marginal increase when additional loss terms are considered. This highlights the models potential for rapid evaluation of network states, particularly in applications requiring repeated or real-time predictions.


\section{Results and Discussion}

This section presents the evaluation of the proposed CensNet-based model and its comparison with a traditional optimization model (APOPT) for two gas transportation networks: a synthetic 8-node network and a real-world 63-node network representing the Colombian gas system. Two experimental settings are analyzed in each case: one that considers only nodal flow prediction loss (CensNet (N)), and another that incorporates both nodal and edge flow losses (CensNet (N+E)). For each configuration, hyperparameter tuning was performed to optimize the performance of the CensNet models. In the 8-node network, the best hyperparameters for the CensNet (N) configuration were $N \  channels=21$, $N \  layers=5$, and $N \  dense=4$, while for CensNet (N+E), the optimal values were $N \  channels=44$, $N \  layers=4$, and $N \  dense=4$. In the case of the 63-node Colombian network, the chosen hyperparameters for the CensNet (N) configuration were $N \  channels=43$, $N \  layers=2$, and $N \  dense=2$, and for CensNet (N+E), the hyperparameters were $N \  channels=25$, $N \  layers=5$, and $N \  dense=4$. These configurations were selected based on the minimal validation losses obtained during training.

\subsection{Prediction Accuracy}

\begin{figure}
    \centering
    \setlength\figurewidth{.53\textwidth}        
    \setlength\figureheight{0.36\textwidth} 
    \subfloat[Nodal flows.] 
    {\label{fig:results_dummy_base_node}\resizebox{\figurewidth}{\figureheight}{\input{figures/Chapter_LinealCensnet/yn_dummy_base.tex}}}
    \subfloat[Edge flows.] 
    {\label{fig:results_dummy_base_f}\resizebox{\figurewidth}{\figureheight}{\input{figures/Chapter_LinealCensnet/yn_dummy_f.tex}}}
    
    \caption{Model results using only the loss associated with nodal flow predictions in the 8-node network.}
    \label{fig:dummy_base_results}
\end{figure}

\begin{figure}
    \centering
        \setlength\figurewidth{.53\textwidth}        
        \setlength\figureheight{0.36\textwidth} 
        \subfloat [Nodal flows.] {\label{fig:yn_col_base}\resizebox{\figurewidth}{\figureheight}{\input{figures/Chapter_LinealCensnet/yn_col_base.tex}}}
        \subfloat [Edge flows.] {\label{fig:ye_col_base}\resizebox{\figurewidth}{\figureheight}{\input{figures/Chapter_LinealCensnet/ye_col_base.tex}}} 
    \caption{Model results using only the loss associated with nodal flow predictions in the colombian 63-node network.}
    \label{fig:lineal_col_base_results}
\end{figure}

Across both networks, the CensNet model effectively identifies which nodes inject gas into the system. Even when trained using only nodal losses, the model achieves high $R^2$ scores for nodal flow predictions (0.988), demonstrating its ability to generalize the mapping between inputs and nodal injections. This trend holds for both the 8-node and 63-node networks, where scatter plots (\Cref{fig:dummy_base_results,fig:lineal_col_base_results}) confirm the alignment between actual and predicted values.

For the 8-node network, where gas injection occurs only at a single node, the CensNet (N) model effectively captures this behavior, with minor deviations from the actual values. Similarly, for the 63-node Colombian network, the model successfully identifies the gas injection points, demonstrating a high degree of accuracy with an $R^2$ value of 0.996. These results validate the model's ability to generalize well even when dealing with larger-scale networks.

\begin{figure}
    \centering
        \setlength\figurewidth{.53\textwidth}        
        \setlength\figureheight{0.36\textwidth} 
        \subfloat [Nodal flows.] {\label{fig:results_dummy_node_base_f}\resizebox{\figurewidth}{\figureheight}{\input{figures/Chapter_LinealCensnet/yn_dummy_base_f.tex}}}
        \subfloat [Edge flows.] {\label{fig:results_dummy_edge_base_f}\resizebox{\figurewidth}{\figureheight}{\input{figures/Chapter_LinealCensnet/ye_dummy_base_f.tex}}}
        \caption{Model results using the losses associated with the flows in nodes and edges predictions in the 8-node network.}
        \label{fig:dummy_base_f_results}
\end{figure}


\begin{figure}
    \centering
        \setlength\figurewidth{.53\textwidth}        
        \setlength\figureheight{0.36\textwidth} 
        \subfloat [Actual vs predicted nodal flows.] {\label{fig:yn_col_base_f}\resizebox{\figurewidth}{\figureheight}{\input{figures/Chapter_LinealCensnet/yn_col_base_f.tex}}}
        \subfloat [Actual vs predicted edge flows.] {\label{fig:ye_col_base_f}\resizebox{\figurewidth}{\figureheight}{\input{figures/Chapter_LinealCensnet/ye_col_base_f.tex}}}
        % \includegraphics[width=0.45\textwidth]{figures/}
        \caption{Model results using the losses associated with the predicted flows injected at the nodes and transported by the pipelines and compressors in the colombian 63-node network.}
        \label{fig:col_base_f_results}
\end{figure}


However, edge flow predictions show a clear distinction between the two training regimes. In both networks, the CensNet model trained only on nodal loss performs poorly in capturing edge-level flow dynamics, as seen in \Cref{fig:results_dummy_base_f,fig:ye_col_base}. This is expected since edge flow behavior was not directly optimized in these cases. Once edge loss is incorporated (Censnet (N+E)), prediction accuracy improves dramatically, as shown by the near-perfect alignment in the edge flow scatter plots (\Cref{fig:dummy_base_f_results,fig:col_base_f_results}). In the 8-node network, the $R^2$ score for edge flow predictions reaches 0.999, while the 63-node case achieves an $R^2$ of 0.996, indicating consistent model generalization across both small and large-scale networks.

The improvements in edge flow predictions are particularly notable in the Colombian network, where the inclusion of edge flow loss helps the model capture the complexities of the pipeline and compressor dynamics, which were previously overlooked in the nodal-only model. This improved prediction for edge flows also demonstrates the model's capability to scale effectively to larger and more complex systems.

\subsection{Gas Balance Consistency}


\begin{table}
    \centering
    \begin{tabular}{|l|c|c|c|c|}
        \hline
        Method & Node Error & Edge Error & Balance Error & Time \\ \hline
        CensNet (N) & \( 6.48 \pm 50.54 \) & \( 62.92 \pm 83.11 \) & \( 3.53 \pm 34.22 \) & \( 13.51 \pm 2.86 \) \\ \hline
        CensNet (N+E) & \( 6.55 \pm 50.51 \) & \( 39.24 \pm 84.29 \) & \( -2.10 \pm 29.80 \) & \( 13.50 \pm 2.86 \) \\ \hline
    \end{tabular}
    \caption{Differences in mean and standard deviation between APOPT and CensNet-based models for node error, edge error, balance error, and prediction time. Each value represents the difference APOPT - CensNet, computed over 100 samples.}
    \label{tab:lineal_dummy_results_diffs}
\end{table}


\begin{table}
    \centering
    \begin{tabular}{|l|c|c|c|c|}
    \hline
    Method & Node Error & Edge Error & Balance Error & Time \\ \hline
    CensNet (N) & \( -0.06 \pm 69.31 \) & \( 62.47 \pm 83.12 \) & \( 0.01 \pm 65.70 \) & \( 13.52 \pm 2.86 \) \\ \hline
    CensNet (N+E) & \( -0.08 \pm 69.20 \) & \( -0.07 \pm 116.32 \) & \( -0.02 \pm 34.09 \) & \( 13.51 \pm 2.86 \) \\ \hline
    \end{tabular}
    \caption{Differences in mean and standard deviation between APOPT and CensNet-based models for node error, edge error, balance error, and prediction time, across 100 samples. Each value represents the difference APOPT - CensNet.}
    \label{tab:lineal_col_results_diffs}
\end{table}



When compared to the APOPT optimization model, the CensNet achieves comparable performance in terms of flow predictions, particularly when both nodal and edge losses are used. In the 8-node case, for instance, the CensNet (N+E) model achieves a significant reduction in edge error compared to the nodal-only model, with a mean edge difference of \(39.24\) units versus \(62.92\), as shown in \Cref{tab:lineal_dummy_results_diffs}. Additionally, the gas balance error improves from \(3.53\) to \(-2.10\), indicating better gas conservation when edge information is incorporated into training.

For the 63-node case, the benefit of including edge losses becomes even more evident. While the nodal-only model (CensNet N) shows a large discrepancy in edge error (\(62.47\)), the CensNet (N+E) model nearly eliminates this difference, achieving a mean edge error of \(-0.07\), as shown in \Cref{tab:lineal_col_results_diffs}. Moreover, the balance error decreases from \(0.01\) to \(-0.02\), again suggesting enhanced physical consistency through the use of comprehensive loss functions.

In terms of nodal predictions, the differences between APOPT and CensNet remain small in both networks. The maximum difference is under \(0.1\) units, which confirms the CensNet's capability to match optimization-based predictions for node flows when trained appropriately.

Regarding computational performance, the CensNet-based model significantly outperforms the optimization model. T-tests were conducted to compare the prediction times for the APOPT and CensNet models across 100 unseen cases. These tests revealed statistically significant differences in computational time between the two models, with the CensNet-based model taking considerably less time to generate predictions. For the 8-node network, the t-test returned a T-statistic of 14.94, with 198 degrees of freedom and a p-value of $1.32 \times 10^{-34}$. Similarly, for the 63-node network, the t-test showed a T-statistic of 47.29 and a p-value of $4.92 \times 10^{-110}$. These results confirm the significant computational advantage of the CensNet-based model over the optimization approach.



\section{Discussion}

Aligned with the first objective, this section discusses the effectiveness of a Graph Neural Networks-based regression approach that leverages the topology of natural gas networks to reduce computation time for operation estimation. In this section, we analyze and summarize the performance of the proposed CensNet-based model in predicting the operational parameters of natural gas transportation systems. The evaluation was conducted on two test cases: a simplified 8-node network and a more complex real-world system with 63 nodes representing the Colombian natural gas system. The results demonstrate the CensNet model's ability to deliver accurate predictions with significant improvements in computational efficiency compared to traditional optimization methods.

A key insight from the experiments is that incorporating both nodal and edge flow losses in the training objective is important to achieving physically consistent and accurate predictions. While the model trained using only nodal losses was able to identify gas injection nodes with high $R^2$ scoresup to 0.996 in both systemsits inability to enforce edge-level flow consistency led to significant errors in gas balance and edge flow predictions. This limitation is expected, as edge flows are governed by physical constraints not captured by nodal outputs alone. Once edge-related losses were included (CensNet (N+E)), the model achieved near-perfect alignment with the actual data for both nodal and edge flows, as confirmed by scatter plots and $R^2$ values reaching 0.999 for edge predictions (see \Cref{fig:dummy_base_f_results,fig:col_base_f_results}). Another contribution is the ability of the CensNet-based model to generalize across different network scales and topologies using topology-informed learning. Despite the increased complexity of the 63-node system, the model retained its predictive accuracy and computational advantages, highlighting the scalability of the approach.

In terms of computational performance, the CensNet model consistently outperformed the optimization-based approach. T-tests applied to prediction times in both experiments confirmed statistically significant differences, with p-values smaller than $10^{-30}$ in all cases. For instance, in the 8-node experiments, the CensNet achieved prediction times under 0.15 seconds on average, compared to over 0.79 seconds for the optimizer, with a T-statistic of $14.94$ and p-value of $1.32 \times 10^{-34}$ in the base case (see \Cref{tab:lineal_dummy_results}). Similar results were observed in the 63-node system, where the CensNet achieved an average prediction time of 0.14 seconds versus 5.01 seconds for the optimizer. The T-statistic of $47.29$ and p-value of $4.92 \times 10^{-110}$ (see \Cref{tab:lineal_col_results}) confirm the robustness of the computational advantage.

Overall, the experiments provide validation that the inclusion of physical losses, combined with GNN architectures that respect the topological structure of gas networks, leads to models that are both accurate and efficient. This supports the premise that GNNs, when appropriately guided by physical insights, can serve as powerful surrogates for traditional optimization tools in the context of operational estimation for natural gas systems.


% \section{Discussion}
%
% Aligned with the first objective, this section discusses the effectiveness of a Graph Neural Networks-based regression approach that leverages the topology of natural gas networks to reduce computation time for operation estimation. This section analyzes the performance of the proposed model in predicting the operational parameters of natural gas transportation systems. The evaluation was conducted on two test cases: a simplified 8-node network and a more complex real-world system with 63 nodes representing the Colombian natural gas system. The results demonstrate the GNN model's ability to make suitable accurate predictions with significant improvements in computational efficiency compared to traditional optimization methods.
%
% The 8-node network provided a controlled environment to test the GNN's predictive capabilities. The initial experiment, where only nodal losses were considered, resulted in a MSE of 1.98. Although the model accurately identified the generating node, its predictions were more concentrated, lacking the dispersion seen in the actual values. This behavior suggests that while the GNN could capture certain trends in gas generation at the nodes, its predictive distribution did not fully align with real-world variability. However, the prediction of edge flows showed significant deviation from the actual values due to the exclusion of edge-related losses in the training process. This underperformance was expected, given this initial model's lack of focus on edge dynamics.
%
% The introduction of edge-related losses in the second stage of the experiment significantly improved the GNN's performance. With optimized hyperparameters, the GNN achieved an overall loss of 2.02, reflecting its enhanced ability to predict both nodal and edge flows. Furthermore, the MSE associated with flow balancing at nodes dropped dramatically, from 284,764 to 1,719, highlighting the importance of including edge losses in the training objective. The comparison between scatter plots in \Cref{fig:dummy_base_results} and \Cref{fig:dummy_base_f_results} confirmed this improvement, with an $R^2$ of 0.999 for edge flow predictions, nearly aligning with the actual data.
%
%  The results presented in \Cref{tab:lineal_dummy_results} illustrate the excellent performance of the GNN-based model, making a trade-off in terms of computational speed and prediction accuracy. The GNN-based model demonstrated advantages over the optimization model regarding computational efficiency, as confirmed by the t-test results. When comparing the time required to generate predictions using node losses only, the GNN-based model outperformed the optimization model. The t-test produced a T-statistic of $14.94$, with $198$ degrees of freedom and a p-value of $1.32 \times 10^{-34}$, indicating a computational advantage for the GNN. On average, the GNN required $0.144$ seconds to make predictions in the Base experiment, compared to $0.792$ seconds for the optimization model. Similar results were observed in the Base-f experiment, where the GNN took $0.145$ seconds, while the optimization model maintained the same prediction time of $0.792$ seconds.
%
% In the second evaluation, considering both node and edge losses, the GNN-based model again outperformed the optimization model. The t-test revealed a T-statistic of $14.81$, with $198$ degrees of freedom and a p-value of $3.47 \times 10^{-34}$, further reinforcing the computational advantage of the GNN. Despite the additional complexity introduced by edge flows, the GNN completed predictions faster than the optimization model, maintaining an average time of $0.144$ seconds in the Base experiment and $0.145$ seconds in the Base-f experiment, compared to the optimization model's consistent time of $0.792$ seconds.
%
% The performance of the GNN-based model was evaluated on the more complex 63-node Colombian natural gas system. Initially, the model was optimized using only the nodal loss, resulting in a total loss of $11.38$ for nodal flows. However, its ability to capture the gas balance was less accurate, with a balance loss of $-2.19 \pm 58.56$. This is due to the exclusion of edge flows during optimization, similar to the first experiment on the 8-node network.
%
% Despite this limitation, the GNN demonstrated strong predictive accuracy for nodal flows, with an $R^2$ value of 0.996, as seen in \Cref{fig:lineal_col_base_results}. This high correlation shows the GNN's ability to predict nodal gas injections effectively, identifying which nodes had active injections. However, since edge flows were not considered in the loss function, predictions for edge flows were less accurate.
%
% The computational efficiency of the GNN-based model was significant. As shown in \Cref{tab:lineal_col_results}, the GNN completed predictions in an average of $0.13$ seconds, compared to $5.01$ seconds for the optimization model. The t-test confirmed this advantage, with a T-statistic of $47.29$ and a p-value of $4.92 \times 10^{-110}$.
%
% In the second experiment, where the loss function included edge flows, the GNN-based model's performance improved significantly for edge predictions. \Cref{fig:col_base_f_results} shows that including edge-related losses allowed the GNN to achieve an $R^2$ value of 0.996 for edge flows, closely matching the actual data. This improvement highlights the importance of accounting for edge flows to enhance overall model accuracy.
%
% The comparison between the optimization and GNN-based models in \Cref{tab:lineal_col_results} shows that both approaches yielded nearly identical mean and standard deviation values for nodal and edge flows. However, the GNN model achieved these results with much greater computational efficiency. The GNN consistently required less time to generate predictions, averaging $0.14$ seconds in the Base-f experiment, compared to the optimization model's $5.01$ seconds. This efficiency makes the GNN a suitable choice for real-time or large-scale applications.
%
