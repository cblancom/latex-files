\chapter{Natural Gas System Prediction Using Graph Neural Networks} \label{cap:lienal-censnet}

\section{Preliminaries}

\subsection{Graph definition}


A graph $G$ is a mathematical structure that represents a set of interconnected objects. These objects are known as vertices (or nodes), denoted by the set $V(G)$, and the connections between them are called edges (or arcs), denoted by the set $E(G)$. Formally, a graph is defined as an ordered pair $G = (V, E)$, where $V(G)$ is a non-empty set of vertices, and $E(G) \subseteq \{(u, v) \mid u, v \in V(G), u \neq v\}$ is a set of edges, where each edge connects two distinct vertices \cite{Trudeau_2015}.

Graphs can be categorized based on the properties of their edges. An undirected graph has edges that do not have a direction, so the pair $(u, v) = (v, u)$ represents an edge that simply connects vertices $u$ and $v$. In contrast, in a directed graph (or digraph), each edge $(u, v) \in E(G)$ has a direction, meaning it goes from vertex $u$ to vertex $v$. This implies that $(u, v) \neq (v, u)$ unless $u = v$ \cite{Bender_Williamson_2010}.

 %
% \begin{figure}
%     \begin{center}
%         \setlength\figurewidth{.5\textwidth}        
%             \setlength\figureheight{0.4\textwidth}
%         \resizebox{\figurewidth}{\figureheight}{\input{figures/Chapter_LinealCensnet/graph_def}}
%         % \input{figures/Chapter_LinealCensnet/graph_def}
%         % \includegraphics[width=0.95\textwidth]{figures/}
%     \end{center}
%     \caption{Directed graph}\label{fig:graph_definition}
% \end{figure}
%
\begin{figure}
    \centering
        \setlength\figurewidth{.53\textwidth}        
        \setlength\figureheight{0.36\textwidth} 
        \subfloat [Undirected graph] {\label{fig:undirected_graph_def}\resizebox{\figurewidth}{\figureheight}{\input{figures/Chapter_LinealCensnet/un_graph_def}}}
        \subfloat [Directed graph] {\label{fig:direc_graph_def}\resizebox{\figurewidth}{\figureheight}{\input{figures/Chapter_LinealCensnet/di_graph_def}}}
        % \includegraphics[width=0.45\textwidth]{figures/}
    \caption{Types of graphs}\label{fig:graph_definition}
\end{figure}

In \cref{fig:graph_definition}, two graphs are represented, each composed of four nodes labeled $1$, $2$, $3$, and $4$, and six edges labeled $A$, $B$, $C$, $D$, $E$, and $F$. The difference between them lies in the type of graph they represent. For example, in \cref{fig:undirected_graph_def}, the edge c shows a connection between nodes 2 and 4. However, in \cref{fig:direc_graph_def}, this connection provides additional information: a direction, which, in the context of this study, could represent the direction of a specific element, such as electric power or gas flow.


A graph can be represented in various ways using matrices, each capturing different aspects of the graph's structure. The two most common matrix representations are the adjacency matrix and the incidence matrix.

The adjacency matrix of a graph is a square matrix used to represent the connections between vertices \cite{wilson_1972}. For a graph $G$ with $n$ vertices, the adjacency matrix $A$ is an $n \times n$ matrix where the entry $a_{ij}$ is defined as follows:

\begin{equation}
 a_{ij} = 
\begin{cases}
1 & \text{if there is an edge from vertex } i \text{ to vertex } j, \\
0 & \text{otherwise}.
\end{cases}
    \label{eq:adjacency_matrix_definition}
\end{equation}

% \[
% a_{ij} =
% \begin{cases}
% 1 & \text{if there is an edge from vertex } i \text{ to vertex } j, \\
% 0 & \text{otherwise}.
% \end{cases}
% \]

For a directed graph, the adjacency matrix captures the direction of the edges. Below is the adjacency matrix for the directed graph shown earlier:

\[
A = \begin{pmatrix}
0 & 1 & 0 & 1 \\
1 & 0 & 0 & 1 \\
0 & 1 & 0 & 1 \\
1 & 1 & 1 & 0
\end{pmatrix}
\]


The incidence matrix of a graph represents the relationship between vertices and edges \cite{wilson_1972}. For a graph $G$ with $n$ vertices and $m$ edges, the incidence matrix $I$ is an $n \times m$ matrix where the entry $i_{ij}$ is defined as follows:

\begin{equation}
 B_{ij} =
\begin{cases}
1 & \text{if vertex } i \text{ is the starting point of edge } j \text{ in a directed graph}, \\
-1 & \text{if vertex } i \text{ is the endpoint of edge } j \text{ in a directed graph}, \\
0 & \text{if vertex } i \text{ is not connected to edge } j.
\end{cases}  
    \label{eq:incidence_matrix_definition}
\end{equation}


For the directed graph previously described, the incidence matrix is given by:

\[
B = \begin{pmatrix}
1 & -1 & 0 & 0 \\
-1 & 0 & -1 & 0 \\
0 & 0 & 0 & 1  \\
0 & 1 & 1 & -1 
\end{pmatrix}
\]

\subsection{Neural networks}

\subsubsection{Multi-Layered Perceptrons}

A Multilayer Perceptron (MLP) is a fundamental type of artificial neural network, often regarded as one of the building blocks of deep learning. At its core, an MLP consists of multiple layers of nodes, or neurons, where each layer is fully connected to the next one. The architecture typically includes an input layer, one or more hidden layers, and an output layer, as can be seen in the \cref{fig:FCNN_representation}.


\begin{figure}
    \centering
    \setlength\figurewidth{1\textwidth}        
    \setlength\figureheight{0.5\textwidth}
    \resizebox{\figurewidth}{\figureheight}{\input{figures/Chapter_LinealCensnet/MLP_def.tex}}
    % \input{figures/Chapter_LinealCensnet/MLP_def.tex}
    \caption{General diagram of a multilayer perceptron, showing the input layers in green, the hidden layers in blue and the outputs in red.}\label{fig:FCNN_representation}
\end{figure}



The neurons in each layer are connected to the neurons in the subsequent layer through weighted connections, the key parameters learned during the training process \cite{Hippert_Pedreira_Souza_2001}. One of the most significant properties of an MLP is its ability to function as a universal approximator. Given sufficient neurons in the hidden layers, an MLP can approximate any continuous function to an arbitrary degree of accuracy, provided the network is trained correctly \cite{Zhang_Eddy_Patuwo_Y_Hu_1998}.

Mathematically, an MLP can be defined as follows. Let $\mathbf{x} \in \mathbb{R}^n$ represent the input vector, where $n$ is the number of features. The output of each neuron in the first hidden layer is calculated as:

\begin{equation}
 \mathbf{z}^{(1)} = \sigma\left(\mathbf{W}^{(1)}\mathbf{x} + \mathbf{b}^{(1)}\right)   
    \label{eq:output_neuron}
\end{equation}



where $\mathbf{W}^{(1)} \in \mathbb{R}^{m_1 \times n}$ is the weight matrix for the first hidden layer, with $m_1$ being the number of neurons in this layer, $\mathbf{b}^{(1)} \in \mathbb{R}^{m_1}$ is the bias vector, and $\sigma(\cdot)$ is the activation function, typically a non-linear function such as the ReLU (Rectified Linear Unit) or sigmoid function.

This process is repeated for each subsequent hidden layer $k$, where the output of the $k$-th layer is given by:

\begin{equation}
 \mathbf{z}^{(k)} = \sigma\left(\mathbf{W}^{(k)}\mathbf{z}^{(k-1)} + \mathbf{b}^{(k)}\right) 
    \label{eq:hidden_layers_output}
\end{equation}


Here, $\mathbf{W}^{(k)} \in \mathbb{R}^{m_k \times m_{k-1}}$ represents the weight matrix connecting layer $k-1$ to layer $k$, $\mathbf{b}^{(k)} \in \mathbb{R}^{m_k}$ is the bias vector for layer $k$, and $\mathbf{z}^{(k-1)}$ is the output of the previous layer.

Finally, the output layer produces the final prediction $\mathbf{\hat{y}}$:

\begin{equation}
 \mathbf{\hat{y}} = \sigma\left(\mathbf{W}^{(L)}\mathbf{z}^{(L-1)} + \mathbf{b}^{(L)}\right)   
    \label{eq:output_layer}
\end{equation}

where $L$ denotes the number of layers in the network, including the input and output layers. Depending on the nature of the problem (e.g., classification or regression), the activation function $\sigma(\cdot)$ used in the output layer can vary, with softmax being common in multi-class classification problems, and a linear activation for regression tasks. The entire MLP is trained using a process called backpropagation, combined with an optimization algorithm like gradient descent, to minimize a loss function $J(\mathbf{y}, \mathbf{\hat{y}})$, which measures the difference between the true outputs $\mathbf{y}$ and the predicted outputs $\mathbf{\hat{y}}$. 


\subsubsection{Graph Neural Networks}

In recent years, Graph Neural Networks (GNNs) have emerged as a powerful tool in machine learning, particularly for tasks involving data that can be naturally represented as graphs. Graphs are a universal data structure that can model various systems in numerous fields, including social networks, biological networks, knowledge graphs, and physical systems. Because of their ability to represent relationships and interactions between entities, graphs are used extensively to model complex structures where the data points are not independent but interconnected \cite{Jia_Wang_Shou_Hosseini_Bai_2023}.

GNNs are important because they can directly operate on graph-structured data, extending neural networks' success from grid-like data structures, such as images and sequences, to more general and irregular structures \cite{Jia_Wang_Shou_Hosseini_Bai_2023a} . Traditional neural networks, like Convolutional Neural Networks (CNNs) or Recurrent Neural Networks (RNNs), are designed to work with data that has a fixed structure. However, many real-world problems involve data that can be better described by graphs, where nodes represent entities and edges represent relationships between those entities \cite{Gupta_Matta_Pant_2021}. 

Graph Neural Networks can be broadly defined as a class of neural networks designed to perform inference on data described by graphs. Formally, let $G = (V, E)$ represent a graph, where $V$ is the set of nodes (or vertices) and $E$ is the set of edges. Each node $v \in V$ can be associated with a feature vector $\mathbf{x}_v$, and each edge $(u, v) \in E$ may have an associated weight or feature vector $\mathbf{e}_{uv}$. The goal of a GNN is to learn a representation for each node (or sometimes for the entire graph) by aggregating and transforming the feature information from the node's local neighborhood in the graph.


In the GNN framework, message passing is understood as a series of iterations in which each node updates its representation by exchanging information with its neighbors. To move from the abstract concept to a practical implementation, the specific functions used for updating and aggregating node features must be defined \cite{Liu_Wu_Liu_Hu_2021}.


The basic message passing operation, which simplifies the original GNN model proposed by \cite{GRLB_Hamilton}, is expressed by the following equation:
\begin{equation}
 \mathbf{h}_u^{(k)} = \sigma\left( \mathbf{W}_{\text{self}}^{(k)} \mathbf{h}_u^{(k-1)} + \mathbf{W}_{\text{neigh}}^{(k)} \sum_{v \in \mathcal{N}(u)} \mathbf{h}_v^{(k-1)} + \mathbf{b}^{(k)} \right)  
    \label{eq:message_passing}
\end{equation}

In this equation:
\begin{itemize}
    \item \( \mathbf{h}_u^{(k)} \) represents the updated feature vector of node \( u \) at layer \( k \).
    \item The term \( \mathbf{W}_{\text{self}}^{(k)} \mathbf{h}_u^{(k-1)} \) applies a transformation to the node's own feature vector from the previous layer, enabling the node to retain and modify its self-information. 
    \item The term \( \mathbf{W}_{\text{neigh}}^{(k)} \sum_{v \in \mathcal{N}(u)} \mathbf{h}_v^{(k-1)} \) aggregates the feature vectors of the neighboring nodes \( v \) in the set \( \mathcal{N}(u) \), and then applies a transformation via the weight matrix \( \mathbf{W}_{\text{neigh}}^{(k)} \) 
    \item \( \mathbf{b}^{(k)} \) is a bias term that can be added to the weighted sum, though it is sometimes omitted for simplicity. 
    \item  The non-linear function \( \sigma(\cdot) \), such as ReLU or tanh, is applied elementwise to introduce non-linearity into the model, which is essential for capturing complex patterns in the data.

\end{itemize}

% . This aggregation step is crucial as it allows the node to incorporate information from its local neighborhood.


In the context of Graph Neural Networks (GNNs), a Graph Convolutional Network (GCN) is a specialized model that applies the concept of convolution, widely used in image processing, to graphs. First introduced by \cite{Kipf:2016tc}, GCNs offer a method to perform deep learning on graph-structured data by extending traditional convolution operations to the irregular domain of graphs.

The fundamental idea behind GCNs is to create a spectral filter that operates on graph data. The filter's purpose is to combine features from a node's local neighborhood, taking into account the graph's structure. This process is mathematically formalized in the following way:

\begin{equation}
 \mathbf{H} = \sigma\left( \tilde{\mathbf{D}}^{-\frac{1}{2}} \tilde{\mathbf{A}} \tilde{\mathbf{D}}^{-\frac{1}{2}} \mathbf{X} \mathbf{\Theta} \right)   
    \label{eq:GCN_filter}
\end{equation}

Where:
\begin{itemize}
    \item \( \mathbf{H} \) represents the matrix of node representations after applying the GCN layer. Each row \( \mathbf{h}_u \) in \( \mathbf{H} \) corresponds to the updated feature vector for node \( u \). 
    \item \( \mathbf{X} \) is the matrix of input node features, where each row \( \mathbf{x}_u \) corresponds to the feature vector for node \( u \) before applying the GCN layer. 
    \item \( \sigma(\cdot) \) denotes a non-linear activation function, such as ReLU, applied elementwise to introduce non-linearity into the model. 
    \item \( \tilde{\mathbf{A}} \) is the adjacency matrix of the graph, with added self-loops to account for the node itself in the aggregation. 
    \item \( \tilde{\mathbf{D}} \) is the degree matrix of the graph, modified to include the self-loops. The degree matrix is diagonal, with each diagonal entry \( \tilde{D}_{ii} \) representing the degree of node \( i \) in the graph. 
    \item \( \mathbf{\Theta} \) is a matrix of trainable parameters, which is learned during the training process to optimize the model's performance.


\end{itemize}

The expression \( \tilde{\mathbf{D}}^{-\frac{1}{2}} \tilde{\mathbf{A}} \tilde{\mathbf{D}}^{-\frac{1}{2}} \) is a normalized version of the adjacency matrix, ensuring that the eigenvalues of the operation are bounded between 0 and 1. This normalization step is crucial as it prevents issues such as exploding or vanishing gradients during the training of deep networks.

Specifically, the adjacency matrix \( \tilde{\mathbf{A}} \) is defined as:

\begin{equation}
 \tilde{\mathbf{A}} = \mathbf{A} + \mathbf{I}   
    \label{eq:modified_adjacency_matrix}
\end{equation}

where \( \mathbf{A} \) is the original adjacency matrix, and \( \mathbf{I} \) is the identity matrix. The identity matrix \( \mathbf{I} \) ensures that each node considers its own features when aggregating information from its neighbors.

The degree matrix \( \tilde{\mathbf{D}} \) is defined as:

\begin{equation}
 \tilde{D}_{ii} = \sum_{j \in V} \tilde{A}_{ij}   
    \label{eq:degree_matrix}
\end{equation}
%
where \( V \) represents the set of all nodes in the graph. The diagonal entries of \( \tilde{\mathbf{D}} \) correspond to the degree of each node, adjusted to account for the added self-loops.

\subsubsection{Convolution with Edge-Node Switching (CensNet)}

Graph Convolutional Networks (GCNs) have demonstrated considerable success in various graph-based machine learning tasks, particularly in their ability to generalize convolution operations to non-Euclidean data structures like graphs \cite{Fu_Wang_Liu_Liu_Zhou_You_Peng_Jing_2022} . GCNs operate by aggregating features from a node's neighbors, thereby capturing local neighborhood information and propagating it through the network layers. Despite their effectiveness, GCNs possess certain limitations that hinder their performance in more complex scenarios.

One notable limitation of GCNs is their reliance solely on node features during the convolution process. This approach disregards the information contained within edge features. By neglecting edge features, GCNs fail to fully exploit the underlying structure of the graph, potentially missing out on critical insights that could enhance model performance \cite{8954414}.

Furthermore, GCNs typically aggregate information from immediate neighbors only, which can limit their ability to capture long-range dependencies in large or densely connected graphs. This restriction can lead to an oversimplified representation of the graph structure, particularly in cases where the graph contains intricate patterns that require deeper and more nuanced analysis \cite{Guo_Zhang_Teng_Lu_2019}.

To overcome the limitations of traditional GCNs, which focus primarily on node features, CensNet introduces a novel approach that integrates both node and edge features into the graph convolution process. The CensNet framework consists of two primary types of layers: the \textit{node layer} and the \textit{edge layer}. These layers work in concert to update node and edge embeddings alternately, leveraging the information from both nodes and edges in the graph \cite{9224195}.

The propagation rules in CensNet are designed to incorporate edge features into the convolution process, enabling a more comprehensive feature propagation across the graph. We define the normalized node adjacency matrix with self-loops as follows:

\begin{equation}
\tilde{\mathbf{A}}_v = \mathbf{D}_v^{-\frac{1}{2}} (\mathbf{A}_v + \mathbf{I}_{N_v}) \mathbf{D}_v^{-\frac{1}{2}},
    \label{eq:normalized_node_adjacency}
\end{equation}

where $\mathbf{D}_v$ is the diagonal degree matrix of $\mathbf{A}_v + \mathbf{I}_{N_v}$.

\paragraph{Node Layer Propagation:}


In the $(l+1)$-th layer, the node features are updated using the following propagation rule:


\begin{equation}
    \mathbf{H}^{(l+1)}_v = \sigma\left(\mathbf{T} \Phi\left(\mathbf{H}^{(l)}_e \mathbf{P}_e\right)\mathbf{T}^\top \odot \tilde{\mathbf{A}}_v \mathbf{H}^{(l)}_v \mathbf{W}_v\right) 
    \label{eq:CensNet_propagation}
\end{equation}

Where, 

\begin{itemize}
    \item \( \mathbf{T} \in \mathbb{R}^{N_v \times N_e} \) is a binary transformation matrix that represents the connections between nodes and edges. Each element \( T_{i,m} \) indicates whether edge \( m \) connects to node \( i \). Specifically, if edge \( m \) is connected to node \( i \), then \( T_{i,m} = 1 \); otherwise, \( T_{i,m} = 0 \). Given that each edge is formed by two nodes, every column of the matrix \( \mathbf{T} \) will have exactly two elements equal to 1, corresponding to the two nodes that the edge connects. 
    \item \( \mathbf{H}^{(l)}_e \) is the edge feature matrix from the \( l \)-th layer. \( \mathbf{P}_e \) is a learnable vector of dimension \( d_e \), which acts as a weight for the edge features. The operation \( \Phi(\mathbf{H}^{(l)}_e \mathbf{P}_e) \) denotes the diagonalization of the vector \( \mathbf{H}^{(l)}_e \mathbf{P}_e \), converting it into a diagonal matrix where the elements of the vector are placed on the diagonal. 
    \item The Hadamard product, denoted by \( \odot \), represents element-wise multiplication between matrices. In this context, it combines the transformed edge features with the node adjacency matrix, integrating information from both the original graph and its line graph.
    \item \( \tilde{\mathbf{A}}_v\) is the normalized adjacency matrix for nodes, as shown in \cref{eq:normalized_node_adjacency}, where \( \mathbf{A}_v \) is the original node adjacency matrix and \( \mathbf{I}_{N_v} \) is the identity matrix that introduces self-loops. This normalization ensures that the contributions from each node's neighbors are appropriately scaled. 
    \item \( \mathbf{H}^{(l)}_v \) represents the node feature matrix from the \( l \)-th layer. \( \mathbf{W}_v \) is a learnable weight matrix that is applied to the node features during the propagation process. 
    \item The activation function \( \sigma \) (typically a non-linear function such as ReLU) is applied element-wise to the resulting matrix to introduce non-linearity into the model.
\end{itemize}


This expression can be understood as a mechanism for integrating node and edge information. The matrix \( \mathbf{T} \) is responsible for transferring edge features into the node domain, allowing these edge-derived features to be merged with the normalized node adjacency matrix \( \tilde{\mathbf{A}}_v \). 



\paragraph{Edge Layer Propagation:}

Similarly, the normalized (Laplacianized) edge adjacency matrix is defined as:

\begin{equation}
    \tilde{\mathbf{A}}_e = \mathbf{D}_e^{-\frac{1}{2}} \left(\mathbf{A}_e + \mathbf{I}_{N_e}\right) \mathbf{D}_e^{-\frac{1}{2}},
    \label{eq:normalized_edge_adjacency}
\end{equation}


where \( D_e \) is the degree matrix corresponding to the edge adjacency matrix \( A_e + I_{N_e} \). The matrix \( \tilde{A}_e \) serves as the normalized version of the edge adjacency matrix, similar to how the node adjacency matrix is normalized. This normalization ensures that the influence of each edge is scaled appropriately, which is crucial for the stability of the propagation process.

The propagation rule for edge features is defined as follows:


\begin{equation}
    \mathbf{H}^{(l+1)}_e = \sigma\left(\mathbf{T}^\top \Phi\left(\mathbf{H}^{(l)}_v \mathbf{P}_v\right) \mathbf{T} \odot \tilde{\mathbf{A}}_e \mathbf{H}^{(l)}_e \mathbf{W}_e\right),
\end{equation}

In this expression, the following components are involved:

\begin{itemize}
    \item \textbf{\( \mathbf{T}^\top \)} is the transpose of the binary transformation matrix \( \mathbf{T} \) used in the node layer propagation. The matrix \( \mathbf{T}^\top \) maps the node features back into the edge domain, allowing the edge features to be updated based on the node information.
    \item \( \mathbf{H}^{(l)}_v \) is the node feature matrix from the \( l \)-th layer, and \( \mathbf{P}_v \) is a learnable weight matrix for the nodes. The operation \( \Phi(\mathbf{H}^{(l)}_v \mathbf{P}_v) \) diagonalizes the product of node features and the learnable weights, similar to the transformation applied to edge features in the node layer propagation.
    \item The matrix \( \tilde{\mathbf{A}}_e \) is the normalized edge adjacency matrix, as defined in \cref{eq:normalized_edge_adjacency}. This matrix integrates information about the connections between edges, analogous to how \( \tilde{\mathbf{A}}_v \) handles connections between nodes.
    \item \( \mathbf{H}^{(l)}_e \) represents the edge feature matrix from the \( l \)-th layer, while \( \mathbf{W}_e \) is a learnable weight matrix that is applied to the edge features during the propagation.
    \item The Hadamard product \( \odot \) element-wise multiplies the transformed node features with the edge adjacency matrix, merging the information from both domains.
    \item As in the node layer propagation, the activation function \( \sigma \) is applied element-wise to introduce non-linearity.
\end{itemize}


This propagation rule updates the edge embeddings by integrating information from the node features and the edge structure, thereby enhancing the expressiveness of the edge representations. The alternating updates between node and edge embeddings allow the model to effectively bridge signals across nodes and edges, leading to more robust and informative graph embeddings.




\subsection{Task-Dependent Loss Functions}

The output layer and corresponding loss functions in CensNet are designed to be task-dependent. For regression tasks, the loss function can be formalized as a regularized mean square error (MSE) loss. The MSE loss measures the difference between the predicted outcomes and the actual continuous values, providing a natural fit for regression problems.


We define the loss function for graph regression as follows:

\begin{equation}
    \mathcal{L}(\Theta) = \sum_{l \in \mathcal{Y}_L} \sum_{f=1}^{F} \| Y_{lf} - \hat{Y}_{lf} \|^2_2 + \lambda \|\Theta\|_p,
\end{equation}

where:

\begin{itemize}
    \item \( Y_{lf} \) represents the true continuous value for the \( f \)-th feature of the \( l \)-th graph in the training set.
    \item \( \hat{Y}_{lf} \) is the predicted outcome generated from the final node hidden layer of the CensNet model.
    \item \( \| Y_{lf} - \hat{Y}_{lf} \|^2_2 \) is the squared difference between the true and predicted values, summed across all features and all graphs in the training set.
    \item \( \lambda \|\Theta\|_p \) is the regularization term, which helps control the model's complexity and prevents overfitting by penalizing large weights. The parameter \( \lambda \) controls the strength of the regularization, while \( p \) determines the type of regularization norm (e.g., \( p=2 \) for \( L_2 \) regularization).
\end{itemize}



\section{Linear formulation of the natural gas system} \label{sec:LinealCensnet_formulation}


Natural gas is a widely used energy resource, particularly for electricity generation. The natural gas system consists of a network of production centers, pipelines, compressor stations, storage facilities, and distribution points that ensure reliable gas delivery from producers to consumers. Mathematically, this system can be represented as a directed graph defined as $\mathcal{G}_f = \left\{\mathcal{N}_f, \mathcal{E}_f\right\}$ where $\mathcal{N}_f$ is the set of units within the gas system, and $ \mathcal{E}_f$ is the set of different elements linking them. This set of units includes gas supply nodes or wells $\mathcal{W} \subset \mathcal{N}_{f}$, gas demand nodes or users $\mathcal{U} \subset \mathcal{N}_{f}$, and gas storage facilities $\mathcal{S} \subset \mathcal{N}_{f}$. Similarly, the set of directed gas adjacency edges $\mathcal{A} = \left\{(n,m) \mid n,m\in\mathcal{N}_f \right\} \subset \mathcal{E}$ delineates the network structure through two kinds of transmission elements: transport pipelines $\mathcal{P} = \left\{p=(n,m) \mid n,m\in\mathcal{N}_f \right\}$ and compressing stations $\mathcal{C} = \left\{c=(n,m) \mid n,m\in\mathcal{N}_f \right\}$, so that $\mathcal{P}\cup\mathcal{C}=\mathcal{A}$ and $\mathcal{P}\cap\mathcal{C}=\emptyset$.


Natural gas transportation requires coordination to manage the flow through the different elements to maintain safe operating ranges. In optimizing this network, mathematical models minimize overall operating costs associated with the various stages of natural gas transportation, compression, storage, and handling unsupplied demand, ensuring compliance with technical and physical constraints. The function is expressed as:

% \begin{equation} \label{eq:obj_func_integrated}
% \begin{split}
% \min_{\mathcal{P}, \mathcal{F}} \quad  \sum_{w \in \mathcal{W}} C_{w}^t {f_{w}^t} + \sum_{p \in \mathcal{P}} C_{p}^t {f_{p}^t} + \sum_{c \in \mathcal{C}} C_{c}^t {f_{c}^t} + \\ \sum_{u \in \mathcal{U}} C_{u}^{t} {f_{u}^{t}} + \quad \sum_{s \in \mathcal{S}} C_{s+}^{t} {f_{s+}^{t}}  + \sum_{s \in \mathcal{S}} C_{s-}^{t} {f_{s-}^{t}} + \sum_{s \in \mathcal{S}} C_{s}^{t} {V_{s}^{t}}
% \end{split}
% \end{equation}

\begin{equation} \label{eq:obj_func_linear_gas}
    \begin{split}
    \min_{\mathcal{P}, \mathcal{F}} \quad  \sum_{w \in \mathcal{W}} C_{w}^t {f_{w}^t} + \sum_{p \in \mathcal{P}} C_{p}^t {f_{p}^t} + \sum_{c \in \mathcal{C}} C_{c}^t {f_{c}^t} + \sum_{u \in \mathcal{U}} C_{u}^{t} {f_{u}^{t}} 
        % + \quad \sum_{s \in \mathcal{S}} C_{s+}^{t} {f_{s+}^{t}}  + \sum_{s \in \mathcal{S}} C_{s-}^{t} {f_{s-}^{t}} + \sum_{s \in \mathcal{S}} C_{s}^{t} {V_{s}^{t}}
    \end{split}
\end{equation}


The term $\sum_{w \in \mathcal{W}} C_{w}^t {f_{w}^t}$ represents the total cost of gas production at the wells, where $C_{w}^t$ denotes the cost per unit flow of gas at a specific well $w$ during time period $t$, and $f_{w}^t$ corresponds to the flow of gas from well $w$. Similarly, the transportation of gas through pipelines is captured by the term $\sum_{p \in \mathcal{P}} C_{p}^t {f_{p}^t}$, where $C_{p}^t$ is the cost per unit flow through pipeline $p$ during time period $t$, and $f_{p}^t$ represents the flow of gas through pipeline $p$. In addition, the total cost associated with gas compression at compressor stations is accounted for by $\sum_{c \in \mathcal{C}} C_{c}^t {f_{c}^t}$, where $C_{c}^t$ is the cost per unit flow at compressor station $c$ during time period $t$, and $f_{c}^t$ is the flow of gas through compressor station $c$.

Beyond production, transportation, and compression, the model also considers the costs related to unmet gas demand. The term $\sum_{u \in \mathcal{U}} C_{u}^{t} {f_{u}^{t}}$ reflects the penalty cost associated with unsupplied gas demand, where $C_{u}^{t}$ is the penalty cost per unit of unsupplied gas at location $u$ during time period $t$, and $f_{u}^{t}$ represents the volume of unmet demand. 



% Additionally, the model includes costs related to storage operations. The term $\sum_{s \in \mathcal{S}} C_{s+}^{t} {f_{s+}^{t}}$ represents the cost of injecting gas into storage facilities, where $C_{s+}^{t}$ is the cost per unit flow into storage during time period $t$, and $f_{s+}^{t}$ denotes the flow into storage at facility $s$. Conversely, the cost of withdrawing gas from storage is captured by $\sum_{s \in \mathcal{S}} C_{s-}^{t} {f_{s-}^{t}}$, where $C_{s-}^{t}$ is the cost per unit flow out of storage during time period $t$, and $f_{s-}^{t}$ represents the flow out of storage at facility $s$. Finally, the model accounts for the storage cost itself through the term $\sum_{s \in \mathcal{S}} C_{s}^{t} {V_{s}^{t}}$, where $C_{s}^{t}$ is the cost per unit volume of gas stored at facility $s$ during time period $t$, and $V_{s}^{t}$ denotes the volume of gas stored. 


% Lastly, \Cref{eq:weymouth_cons}, known as the Weymouth equation, summarizes the physical behavior of gas flow through pipelines by relating the gas flow through the pipeline $f_{p}^t$ to the pressures at the ends of the pipeline $\pi_{n}^t, \pi_{m}^t \ \forall \ p = (n,m) \in\mathcal{P}$. The Weymouth equation defines a nonlinear, nonconvex, disjunctive flow-pressure relationship that hampers the optimization of the gas transport system.
% \begin{subequations}
\begin{alignat}{4}
    \underline{f_{w}^t} \leq f_{w}^t \leq \overline{f_{w}^t} &\quad \forall \ w \in \mathcal{W} \label{eq:well_limits} \\
    -\overline{f_{p}^t} \leq f_{p}^t \leq \overline{f_{p}^t} &\quad \forall \ p \in \mathcal{P} \label{eq:pipe_limits} \\
    % \underline{\pi_{n}^t} \leq \pi_{n}^t \leq \overline{\pi_{n}^t} &\quad \forall \ n \in \mathcal{N}_f \label{eq:press_limit} \\
    % \pi_{m}^t \leq \beta_{c}^t{\pi_{n}^t} &\quad \forall c=(n,m) \in \mathcal{C} \label{eq:comp_ratio} \\
    0 \leq f_{u}^{t} \leq \overline{f_{u}^{t}} &\quad \forall \ u \in \mathcal{U} \label{eq:dem_limit_gas} \\
    \sum_{m:(m,n)\in\mathcal{A}}{f_{m}^t} = \sum_{m':(n,m')\in\mathcal{A}}{f_{m'}^t} &\quad \forall \ n \in \mathcal{N}_f \label{eq:gas_balance}
    % 0 \leq f_{s+}^t \leq V_{0s} - \underline{V_s} &\quad \forall \ s \in \mathcal{S} \label{eq:sto_limit1} \\ 
    % 0 \leq f_{s-}^t \leq \overline{V_s} - V_{0s} &\quad \forall \ s \in \mathcal{S} \label{eq:sto_limit2} \\ 
    % V_{s}^t = V_{s}^{t-1} + f_{s-}^{t-1} - f_{s+}^{t-1} &\quad \forall \ s \in \mathcal{S} \label{eq:sto_time}\\
    % sgn(f_{p}^t)(f_{p}^t)^2 = K_{nm}((\pi_{n}^t)^2-(\pi_{m}^t)^2) &\quad \forall \ p =(n,m) \in\mathcal{P} \label{eq:weymouth_cons}
\end{alignat}

The constraint set models the gas transportation system: \Cref{eq:well_limits} forces each production well to inject the flow $f_{w}^t$ over the technical minimum $\underline{f_{w}^t}$ and under the maximum capacity $\overline{f_{w}^t}$. \Cref{eq:pipe_limits} upper-bounds the gas flow through pipelines $f_{p}^t$ to the structural capacity $\overline{f_{p}^t}$. \Cref{eq:dem_limit_gas} ensures that the unsupplied demand $f_{u}^{t}$ is lower than the corresponding user demand $\overline{f_{u}^{t}}$. The nodal gas balance in \Cref{eq:gas_balance} guarantees that the gas entering the node $n$ equals the gas leaving it. 

% \Cref{eq:sto_limit1,eq:sto_limit2} limit the gas injection $f_{s+}$  and extraction $f_{s-}$ rates at storage facilities according to the feasible operating range determined by the currently stored volume $V_{s}^t$, respectively. In turn, \Cref{eq:sto_time} balances the gas storage unit such that gas volume at operation period $t$ $V_{s}^t$ equals the volume from period $V_{s}^{t-1}$ plus the difference between injected $f_{s+}^{t-1}$ and extracted $f_{s+}^{t-1}$ gas flow, a fundamental constraint for modeling the dynamics of gas storage over time. 

\section{Experimental Setup} \label{sec:LinealCensnet_ExperimentalSetup}



In this experimental setup, we take the optimization model presented in the previous section and generate samples by introducing noise into the base values of two gas networks. The noise levels range from 5\% to 25\%, applied to the parameters of the networks to simulate varying operating conditions. The first network is a small-scale test network consisting of 8 nodes, while the second represents the Colombian natural gas transportation system, a more extensive and complex network. These networks were used to evaluate the performance of the proposed model in different scenarios.

The generated samples served as training data for a GNN-based model to solve the natural gas transportation problem, which was designed as a faster alternative to the optimization-based model. This GNN model is built to focus on predicting node and edge-level characteristics, incorporating the structure of the network and its connectivity into the learning process. The model explicitly penalizes deviations in node and edge losses, which directly impact gas flow efficiency through the system. To achieve this, the architecture is structured as a multi-layer neural network, with customizable depth (number of layers), channels, and dense layers, ensuring flexibility in adapting to small-scale and large-scale networks, such as the Colombian system.



A general outline of the model can be seen in \cref{fig:model_description} and the basic components of the model are explained below:

\begin{itemize}
    \item \textbf{Input Channels:} The model receives five types of input data:
        
        
    \begin{itemize}
        \item Node Features: A matrix $\in \mathbb{R}^{N \times 3}$, where $N$ is the number of nodes in the network, containing the features of each node. Each node feature includes the lower and upper limits for injected flow, as well as demanded flow.
        \item Node Laplacian: An adjacency matrix of size $N\times N$ encoding the graph structure of the nodes.
        \item Edge Laplacian: A matrix of size $E\times E$ encoding the connections between edges.
        \item Incidence Matrix: A matrix of size $N\times E$ representing the node-edge incidence relationship, mapping the flow of gas between nodes through edges.
        \item Edge Features: A matrix $\in \mathbb{R}^{N \times 3}$ that includes the features of pipelines and compressors in the gas network. Each edge feature includes the $K$ constant, the maximum compression ratio $\beta$, and the upper and lower flow limits.
    \end{itemize}

    \item \textbf{Normalization and Pre\-dense Layers:} The node and edge inputs receive feature-wise normalization to standardize the data. Following this, the inputs are passed through two dense layers, each with $N \ channels$ neurons. The purpose of these pre-dense layers is to transform the feature space before applying the convolutional layers 

    \item \textbf{Convolutional Layers:} The main body of the network consists of \(N\) convolutional blocks. Each block applies a CensNet convolution, which updates both node and edge features by considering the structural relationships encoded in the node and edge Laplacians, as well as the incidence matrix. Batch normalization follows each convolution to stabilize learning. This structure allows the model to capture complex interactions between nodes and edges and propagate information across the graph, learning how local features influence the broader system.
   
    \item \textbf{Post-dense Layer:} After passing through the convolutional blocks, the node and edge features are further processed by a series of dense layers. The number of dense layers $N \ dense$ is adjustable. These layers further refine the learned features, enabling the model to output node and edge-level predictions. 
   
    \item \textbf{Losses and Outputs:} The final outputs of the network are the node-level and edge-level predictions. The node predictions correspond to the estimated flow at each node, while the edge predictions represent the flow along the edges. Both outputs are penalized based on their respective losses, which are calculated by comparing the predicted values to ground truth values and evaluating how well the physical constraints are respected.

The loss functions ensure that the model accurately predicts the node and edge flows while satisfying the physical constraints of the system. These constraints are essential for ensuring that the predicted flows are feasible within the operational limitations of the network. 
    \item \textbf{Model Optimization:} The model is trained using backpropagation with the Adam optimizer. The training process involves minimizing the node and edge loss functions, which penalize incorrect flow predictions and deviations from the expected behavior of the network.

\begin{figure}
    \centering
    \setlength\figurewidth{1\textwidth}        
    \setlength\figureheight{0.5\textwidth}
    \resizebox{\figurewidth}{\figureheight}{\input{figures/Chapter_LinealCensnet/model_description.tex}}
    % \input{figures/Chapter_LinealCensnet/MLP_def.tex}
    \caption{General outline of the CensNet-based model used.}
    \label{fig:model_description}
\end{figure}
\end{itemize}


The network training was done under the following conditions: data was partitioned into training, validation, and test sets, with percentages of 60\%, 20\%, and 20\%. The learning rate schedule followed an Exponential Decay approach, with an initial learning rate of $1 \times 10^{-2}$ decay steps of 1000, and a decay rate of 0.9. The model was trained over 1500 epochs, ensuring the parameters had sufficient time to converge for both node- and edge-level predictions using a Leaky Relu activation function with an alpha parameter of 0.2.


The model utilized several key hyperparameters: $N \ channels$, which corresponds to the units in the pre-dense and convolutional layers (CensNet), N Dense, representing the number of post-dense layers, and $N \ layers$, denoting the number of convolutional layers. Additionally, the model incorporated weights to penalize deviations in the network's response, specifically targeting MSE between the actual flow and the predicted flow at both the nodes and pipelines. Two tests were conducted to determine the optimal set of hyperparameters. The first test considered only the weight associated with node flow preconditions, while the second test included weights for both node and pipeline flows. In each test, the hyperparameters were optimized using the open-source Optuna framework \cite{optuna_2019}, with the following search space: $N \ channels$ ranged from 16 to 64, $N \ layers$ from 1 to 5, and $N \ dense$ from 2 to 32.




\section{Results}

In this section, we present the results of the proposed GNN model, focusing on the relationship between the predicted outputs and the actual observed values in the natural gas transportation networks. The evaluation includes both the 8-node test network and the Colombian natural gas system, with the goal of assessing the model's ability to predict key parameters under varying operational conditions. 


%
% \begin{figure}
%     \centering
%     \input{figures/Chapter_LinealCensnet/yn_dummy_base.tex}
%     \caption{Your caption here}\label{fig:your-label-here}
% \end{figure}
%

\subsection{Case Study I: 8-node Network}


As a result of the optimization process for this first test, the following hyperparameters were obtained: $N \ channels=21$, $N \ layers=5$, and $N \ dense=4$. With these optimized values, the model achieved an MSE of 1.98. In \cref{fig:results_dummy_base_node}, a scatter plot illustrates the relationship between the actual values of gas generation at the nodes and the corresponding values predicted by the trained neural network, considering only the losses at the nodes. The plot highlights how effectively the network captures the fact that only one of the nodes in the system has gas generation. However, while the network successfully identifies the generating node, the predicted values exhibit less dispersion than the actual values, indicating that the model's predictions are more concentrated around a certain point.


\Cref{fig:results_dummy_base_f} displays the relationship between the actual gas flows through the edges and the predictions made by the neural network. In this case, the model struggled to predict the flow values accurately, demonstrating a significant deviation from the actual data. This result is not unexpected, as the model used in this experiment focused solely on the losses related to gas flows at the injection nodes without accounting for the gas transported through the edges. As such, the absence of edge-related loss consideration likely contributed to the poor prediction performance for edge flows.


\begin{figure}
    \centering
    \setlength\figurewidth{.53\textwidth}        
    \setlength\figureheight{0.36\textwidth} 
    \subfloat[Actual vs predicted nodal flows.] 
    {\label{fig:results_dummy_base_node}\resizebox{\figurewidth}{\figureheight}{\input{figures/Chapter_LinealCensnet/yn_dummy_base.tex}}}
    \subfloat[Actual vs predicted edge flows.] 
    {\label{fig:results_dummy_base_f}\resizebox{\figurewidth}{\figureheight}{\input{figures/Chapter_LinealCensnet/yn_dummy_f.tex}}}
    
    \caption{Model results using only the loss associated with nodal flow predictions in the 8-node network.}
    \label{fig:dummy_base_results}
\end{figure}



To evaluate the performance of the optimization model and the GNN-based model, a t-test was performed, comparing the time each took to generate 100 predictions for new, unseen cases. The results showed a T-statistic of $14.94$, 198 degrees of freedom and a p-value of 1.32e-34, indicating that the optimization model took significantly longer than the GNN-based model, confirming its higher computational cost.

% The table was here !!

In the next stage of the experiment, the model was updated to account for the loss associated with edge flow predictions. This allowed the network to better capture the dynamics of gas transportation across the entire network, not just at the nodes. In this case, the hyperparameter tuning revealed that the best combination was $N channels=44$, $N Layers=4$, and $N Dense=4$, with a test data loss of 2.02, where 1.996 corresponds to the loss associated with the prediction of flows injected at the nodes, and 0.207 corresponds to the loss related to the prediction of flows through the edges. Additionally, the MSE between the gas balance based on actual flows and predicted flows was calculated, resulting in a loss of 1.719. It is essential to highlight that, in the previous experiment, this same loss was 284.764. This loss was computed outside the training stage, so it did not influence the optimization of parameters or hyperparameters.


\Cref{fig:results_dummy_node_base_f} shows the relationship between the actual injection values at the nodes and the corresponding predictions. As in the previous experiment, the model successfully recognizes that gas injection occurs only at a single node, although the predicted values differ slightly from the actual data. However, a significant improvement is observed when analyzing the flows through the edges. \Cref{fig:results_dummy_edge_base_f} highlights this, where the network's predictions for edge flows are almost perfectly aligned with the actual test data, as evidenced by an $R^2$ of 0.999. The near-perfect scatter plot in \cref{fig:results_dummy_edge_base_f} demonstrates the network's ability to make highly accurate predictions for gas flows through the edges in this updated experiment.



\begin{figure}
    \centering
        \setlength\figurewidth{.53\textwidth}        
        \setlength\figureheight{0.36\textwidth} 
        \subfloat [Actual vs predicted nodal flows.] {\label{fig:results_dummy_node_base_f}\resizebox{\figurewidth}{\figureheight}{\input{figures/Chapter_LinealCensnet/yn_dummy_base_f.tex}}}
        \subfloat [Actual vs predicted edge flows.] {\label{fig:results_dummy_edge_base_f}\resizebox{\figurewidth}{\figureheight}{\input{figures/Chapter_LinealCensnet/ye_dummy_base_f.tex}}}
        \caption{Model results using the losses associated with the flows in nodes and edges predictions in the 8-node network.}
        \label{fig:dummy_base_f_results}
\end{figure}


To evaluate the computational performance when considering the loss associated with pipelines and compressor flows, a t-test was performed comparing the computation times of the optimization model and the GNN-based model. The results showed a T-statistic of $14.81$, $198$ degrees of freedom and a p-value of $3.47e-34$, indicating that the optimization model took significantly longer to compute than the GNN-based model. This confirms the consistently higher computational cost of the optimization model, even when additional complexities, such as pipeline flows, are included in the network.



% \begin{table}
%     \centering
%     \begin{tabular}{|c|c|c|c|c|c|}
%         \hline
%         Method & Source & Node Value & Edge Value & Balance Value & Time \\ \hline
%         Opt & Base   & 4.844 ± 12.874 & 24.221 ± 14.041 & -0.001 ± 0.038 & 0.792 ± 0.436 \\ \hline
%         GNN & Base   & 4.840 ± 12.794 & 0.455 ± 0.262   & -5.711 ± 16.854 & 0.144 ± 0.044 \\ \hline
%         Opt & Base-f & 4.844 ± 12.874 & 24.221 ± 14.041 & -0.001 ± 0.038 & 0.792 ± 0.436 \\ \hline
%         GNN & Base-f & 4.767 ± 12.701 & 24.142 ± 14.083 & -0.079 ± 1.174  & 0.145 ± 0.060 \\ \hline
%     \end{tabular}
%     \caption{Comparison of mean and standard deviation values for nodal flows, edge flows, nodal balance, and prediction time between the optimization model and the GNN-based model, across 100 samples for Base and Base-f.}
%     \label{tab:lineal_dummy_results}
% \end{table}

\begin{table}
    \centering
    \begin{tabular}{|c|p{0.8cm}|p{0.55cm}|c|c|c|c|}
        \hline
        Method & \multicolumn{2}{|c|}{Configuration} & Node Value & Edge Value & Balance Value & Time \\ \hline
               & \centering N & \centering E &  &  &  &  \\ \hline
        APOPT & \makebox[0.8cm]{\centering \checkmark} &   & 4.844 ± 12.874 & 24.221 ± 14.041 & -0.001 ± 0.038 & 0.792 ± 0.436 \\ \hline
        GNN & \makebox[0.8cm]{\centering \checkmark} &   & 4.840 ± 12.794 & 0.455 ± 0.262   & -5.711 ± 16.854 & 0.144 ± 0.044 \\ \hline
        % APOPT & \makebox[0.8cm]{\centering \checkmark} & \makebox[0.55cm]{\centering \checkmark} & 4.844 ± 12.874 & 24.221 ± 14.041 & -0.001 ± 0.038 & 0.792 ± 0.436 \\ \hline
        GNN & \makebox[0.8cm]{\centering \checkmark} & \makebox[0.55cm]{\centering \checkmark} & 4.767 ± 12.701 & 24.142 ± 14.083 & -0.079 ± 1.174  & 0.145 ± 0.060 \\ \hline
    \end{tabular}
    \caption{Comparison of mean and standard deviation values for nodal flows, edge flows, nodal balance, and prediction time between the optimization model and the GNN-based model, across 100 samples. The 'N' column refers to experiments considering the nodal loss, while the 'E' column refers to experiments considering the edge loss.}
    \label{tab:lineal_dummy_results}
\end{table}



The \cref{tab:lineal_dummy_results} presents a combined comparison of the optimization model (APOPT) and the GNN-based model across four experiments. Each experiment has three degrees of freedom: the model method (APOPT or GNN), whether nodal loss (N) was included, and whether edge loss (E) was included. The table focuses on four key metrics: nodal flows, edge flows, nodal balance, and prediction time. Mean and standard deviation values are reported based on 100 samples.


Both models display similar results for nodal flows in the experiments, considering only the nodal loss (N). For instance, the APOPT model consistently records a mean nodal flow of \(4.844 \pm 12.874\), while the GNN shows a slightly lower mean of \(4.767 \pm 12.701\) in experiments where both N and E were considered. 


On the other hand, edge flow (E) values vary more significantly between the models. In experiments with only N, the GNN-based model achieves a much lower mean edge flow of \(0.455 \pm 0.262\) compared to the APOPT model’s \(24.221 \pm 14.041\). However, when both N and E were included, both models showed similar edge flow values (\(24.142 \pm 14.083\) for GNN and \(24.221 \pm 14.041\) for APOPT), demonstrating that under these conditions, the GNN model can match the accuracy of the optimization model in predicting edge flows.


Regarding nodal balance, the APOPT model remains consistent across all experiments, showing a near-zero mean balance. The GNN-based model, however, fluctuates more, especially in the N-only experiment, with a mean balance of \(-5.711 \pm 16.854\), though this variance diminishes to \(-0.079 \pm 1.174\) when both N and E are included.

Finally, the GNN model consistently outperforms APOPT in terms of prediction time. Across all experiments, the GNN-based model achieves much faster predictions (\(0.144\) to \(0.145\) seconds) compared to the APOPT model’s \(0.792\) seconds. This superior computational efficiency makes the GNN model particularly appealing for real-time applications.




\subsection{Case Study II: 63-node Network (Colombia)}


The second case studied corresponds to a gas transportation network consisting of 63 nodes representing the Colombian natural gas system. In the first test, where only the loss associated with the flows injected into the nodes was considered the hyperparameter tuning resulted in the following values: $N \ channels = 43$, $N \ layers = 2$, and $N \ dense = 2$. The total loss obtained was 10.598, with the loss associated with the nodal flows. The gas balance loss was 3426.659, but it is important to note that only the nodal flows were considered during the network optimization, and the balance loss was computed after the training process.

The \cref{fig:yn_col_base} shows the scatter plot relating the actual injected flows at the nodes to the predicted flows. This figure demonstrates the network's ability to accurately identify which nodes had gas injections and which did not. Furthermore, with an $R^2$ value of 0.996, the network effectively captured the correct proportions of these injected flows, indicating a robust predictive performance. 
On the other hand, \cref{fig:ye_col_base} presents the relationship between the actual and predicted flows through the edges, encompassing both gas pipelines and compressors. In this case, the network struggled to predict the flow values accurately for these elements, which is unsurprising given that the loss associated with the edge flows was not included in the network's optimization process for this test.

\begin{figure}
    \centering
        \setlength\figurewidth{.53\textwidth}        
        \setlength\figureheight{0.36\textwidth} 
        \subfloat [Actual vs predicted nodal flows.] {\label{fig:yn_col_base}\resizebox{\figurewidth}{\figureheight}{\input{figures/Chapter_LinealCensnet/yn_col_base.tex}}}
        \subfloat [Actual vs predicted edge flows.] {\label{fig:ye_col_base}\resizebox{\figurewidth}{\figureheight}{\input{figures/Chapter_LinealCensnet/ye_col_base.tex}}} 
    \caption{Model results using only the loss associated with nodal flow predictions in the colombian 63-node network.}
    \label{fig:lineal_col_base_results}
\end{figure}


For the current experiment stage, involving the 63-node Colombian network, a t-test was conducted to compare the computation times between the optimization model and the GNN-based model. The test yielded a T-statistic of $47.29$, 198 degrees of freedom and a p-value of $4.92 \times 10^{-110}$, demonstrating a highly significant difference in the computational times. As with the previous experiments, these results confirm that the GNN-based model significantly outperformed the optimization model in terms of prediction speed.


In the next stage of this case study, predictions were made on the same system, but this time, considering not only the losses associated with the flows injected at the nodes but also those corresponding to the flows transported by the system's pipelines and compressors. For this experiment, the hyperparameter tuning resulted in $N \ channels = 25$, $N \ layers = 5$, and $N \ dense = 4$, with a total loss of 34.399. Of this, 10.792 were associated with the predictions of the flows injected at the nodes, while 23,607 corresponded to the flows transported through the edges. The MSE for the gas balance was 279.754.

\Cref{fig:yn_col_base_f} illustrates that, as in the previous case, the network can predict which nodes can inject natural gas and demonstrate a certain degree of accuracy regarding their injection capacity. The critical difference in this test, compared to the previous one, is shown in \cref{fig:ye_col_base_f}. Here, it is evident that by including the losses associated with the edges of the Colombian transportation network, the model significantly improves its predictions for these elements, achieving an $R^2$ value of 0.996 compared to the actual data.


\begin{figure}
    \centering
        \setlength\figurewidth{.53\textwidth}        
        \setlength\figureheight{0.36\textwidth} 
        \subfloat [Actual vs predicted nodal flows.] {\label{fig:yn_col_base_f}\resizebox{\figurewidth}{\figureheight}{\input{figures/Chapter_LinealCensnet/yn_col_base_f.tex}}}
        \subfloat [Actual vs predicted edge flows.] {\label{fig:ye_col_base_f}\resizebox{\figurewidth}{\figureheight}{\input{figures/Chapter_LinealCensnet/ye_col_base_f.tex}}}
        % \includegraphics[width=0.45\textwidth]{figures/}
        \caption{Model results using the losses associated with the predicted flows injected at the nodes and transported by the pipelines and compressors in the colombian 63-node network.}
        \label{fig:col_base_f_results}
\end{figure}


In this stage, a t-test was performed to compare the computation times between the optimization model and the GNN-based model. The results yielded a T-statistic of $47.25$, $198$ degrees of freedom and a p-value of $4.46 \times 10^{-110}$. These values confirm with high statistical confidence that the GNN-based model significantly outperforms the optimization model in terms of computational efficiency when predicting flows across the network.


\begin{table}[htbp]
\centering
\begin{tabular}{|c|p{0.8cm}|p{0.55cm}|c|c|c|c|}
    \hline
    Method & \multicolumn{2}{|c|}{Experiment} & Node Value & Edge Value & Balance Value & Time \\ \hline
           & \centering N & \centering E &  &  &  &  \\ \hline
    Opt    & \makebox[0.8cm]{\centering \checkmark} &   & 11.41 ± 49.23 & 63.52 ± 81.62 & -2.15 ± 16.49 & 5.01 ± 5.59 \\ \hline
    GNN    & \makebox[0.8cm]{\centering \checkmark} &   & 11.38 ± 49.13 & 0.91 ± 1.26   & -2.19 ± 58.56 & 0.13 ± 0.07 \\ \hline
    Opt    & \makebox[0.8cm]{\centering \checkmark} & \makebox[0.55cm]{\centering \checkmark} & 11.41 ± 49.23 & 63.52 ± 81.62 & -2.15 ± 16.50 & 5.01 ± 5.59 \\ \hline
    GNN    & \makebox[0.8cm]{\centering \checkmark} & \makebox[0.55cm]{\centering \checkmark} & 11.40 ± 48.97 & 63.45 ± 81.38 & -2.16 ± 16.60 & 0.14 ± 0.08 \\ \hline
\end{tabular}
\caption{Comparison of mean and standard deviation values for nodal flows, edge flows, nodal balance, and prediction time between the optimization model (Opt) and the GNN-based model, across 100 samples. The 'N' column refers to experiments considering the nodal loss, and the 'E' column refers to experiments considering the edge loss.}
\label{tab:lineal_col_results}
\end{table}


The \cref{tab:lineal_col_results} presents a comparison between the optimization model (Opt) and the GNN-based model across four experiments, which vary based on the method used (Opt or GNN) and the inclusion of nodal loss (N) and edge loss (E). The comparison focuses on nodal flows, edge flows, nodal balance, and prediction time, with the mean and standard deviation calculated from 100 samples.


In terms of nodal flows, both the optimization model and the GNN-based model show almost identical results across all experiments. For both models, the mean nodal flow remains consistent at around $11.41$, with very similar standard deviations: $49.23$ for the optimization model and $49.13$ and $48.97$ for the GNN model in the experiments considering only nodal loss and both nodal and edge losses, respectively. This consistency highlights that both models perform similarly in predicting nodal flows.


However, the edge flow values present a more notable distinction between the two models. In the experiments without edge loss, the GNN-based model significantly outperforms the optimization model, achieving a much lower mean edge flow of $0.91 \pm 1.26$, compared to the optimization model’s $63.52 \pm 81.62$. This demonstrates the GNN’s improved capability to predict edge flows with reduced variability. In the experiments that include both nodal and edge losses, the edge flow predictions for both models converge, with the GNN reporting $63.45 \pm 81.38$ and the optimization model showing $63.52 \pm 81.62$.



Regarding the nodal balance, the models display similar mean results. In the experiments without edge loss, the optimization model reports a mean balance of $-2.15 \pm 16.49$, while the GNN produces $-2.19 \pm 58.56$. In the experiments considering both nodal and edge losses, the mean balance results become more consistent, with the GNN model showing $-2.16 \pm 16.60$ and the optimization model remaining at $-2.15 \pm 16.50$. The higher variability observed in the GNN’s predictions during the nodal-only experiments is reduced when edge loss is included, indicating improved consistency.

Finally, the prediction time demonstrates the GNN’s computational efficiency. In all experiments, the GNN completes predictions significantly faster than the optimization model. In the nodal-only experiments, the GNN requires just $0.13$ seconds on average, compared to $5.01$ seconds for the optimization model. In the nodal and edge loss experiments, the GNN’s prediction time increases only slightly to $0.14$ seconds, whereas the optimization model maintains the same $5.01$ seconds. This significant reduction in prediction time underscores the GNN’s efficiency in handling large-scale network predictions.

\section{Discussion and conclusions}

In this section, we analyze and summarize the performance of the proposed GNN-basd model in predicting the operational parameters of natural gas transportation systems. The evaluation was conducted on two test cases: a simplified 8-node network and a more complex real-world system with 63 nodes representing the Colombian natural gas system. The results demonstrate the GNN model's ability to deliver accurate predictions with significant improvements in computational efficiency compared to traditional optimization methods.


The 8-node network provided a controlled environment to test the GNN's predictive capabilities. The initial experiment, where only nodal losses were considered, resulted in a MSE of 1.98. Although the model accurately identified the generating node, its predictions were more concentrated, lacking the dispersion seen in the actual values. This behavior suggests that while the GNN could capture certain trends in gas generation at the nodes, its predictive distribution did not fully align with real-world variability. However, the prediction of edge flows showed significant deviation from the actual values due to the exclusion of edge-related losses in the training process. This underperformance was expected, given this initial model's lack of focus on edge dynamics.


The introduction of edge-related losses in the second stage of the experiment significantly improved the GNN's performance. With optimized hyperparameters, the GNN achieved an overall loss of 2.02, reflecting its enhanced ability to predict both nodal and edge flows. Furthermore, the MSE associated with flow balancing at nodes dropped dramatically, from 284,764 to 1,719, highlighting the importance of including edge losses in the training objective. The comparison between scatter plots in \cref{fig:dummy_base_results} and \cref{fig:dummy_base_f_results} confirmed this improvement, with an $R^2$ of 0.999 for edge flow predictions, nearly aligning with the actual data.



 The results presented in \cref{tab:lineal_dummy_results} illustrate the excellent performance of the GNN-based model, making a trade-off in terms of computational speed and prediction accuracy. The GNN-based model demonstrated advantages over the optimization model regarding computational efficiency, as confirmed by the t-test results. When comparing the time required to generate predictions using node losses only, the GNN-based model outperformed the optimization model. The t-test produced a T-statistic of $14.94$, with $198$ degrees of freedom and a p-value of $1.32 \times 10^{-34}$, indicating a computational advantage for the GNN. On average, the GNN required $0.144$ seconds to make predictions in the Base experiment, compared to $0.792$ seconds for the optimization model. Similar results were observed in the Base-f experiment, where the GNN took $0.145$ seconds, while the optimization model maintained the same prediction time of $0.792$ seconds.

In the second evaluation, considering both node and edge losses, the GNN-based model again outperformed the optimization model. The t-test revealed a T-statistic of $14.81$, with $198$ degrees of freedom and a p-value of $3.47 \times 10^{-34}$, further reinforcing the computational advantage of the GNN. Despite the additional complexity introduced by edge flows, the GNN completed predictions faster than the optimization model, maintaining an average time of $0.144$ seconds in the Base experiment and $0.145$ seconds in the Base-f experiment, compared to the optimization model's consistent time of $0.792$ seconds.




The performance of the GNN-based model was evaluated on the more complex 63-node Colombian natural gas system. Initially, the model was optimized using only the nodal loss, resulting in a total loss of $11.38$ for nodal flows. However, its ability to capture the gas balance was less accurate, with a balance loss of $-2.19 \pm 58.56$. This is due to the exclusion of edge flows during optimization, similar to the first experiment on the 8-node network.

Despite this limitation, the GNN demonstrated strong predictive accuracy for nodal flows, with an $R^2$ value of 0.996, as seen in \cref{fig:lineal_col_base_results}. This high correlation shows the GNN's ability to predict nodal gas injections effectively, identifying which nodes had active injections. However, since edge flows were not considered in the loss function, predictions for edge flows were less accurate.

The computational efficiency of the GNN-based model was significant. As shown in \cref{tab:lineal_col_results}, the GNN completed predictions in an average of $0.13$ seconds, compared to $5.01$ seconds for the optimization model. The t-test confirmed this advantage, with a T-statistic of $47.29$ and a p-value of $4.92 \times 10^{-110}$.

In the second experiment, where the loss function included edge flows, the GNN-based model's performance improved significantly for edge predictions. \cref{fig:col_base_f_results} shows that including edge-related losses allowed the GNN to achieve an $R^2$ value of 0.996 for edge flows, closely matching the actual data. This improvement highlights the importance of accounting for edge flows to enhance overall model accuracy.

The comparison between the optimization and GNN-based models in \cref{tab:lineal_col_results} shows that both approaches yielded nearly identical mean and standard deviation values for nodal and edge flows. However, the GNN model achieved these results with much greater computational efficiency. The GNN consistently required less time to generate predictions, averaging $0.14$ seconds in the Base-f experiment, compared to the optimization model's $5.01$ seconds. This efficiency makes the GNN a suitable choice for real-time or large-scale applications.

